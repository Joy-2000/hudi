<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Apache Hudi: User-Facing Analytics</title>
        <link>https://hudi.apache.org/blog</link>
        <description>Apache Hudi Blog</description>
        <lastBuildDate>Wed, 15 Jan 2025 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Out of the box Key Generators in Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi</link>
            <guid>https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi</guid>
            <pubDate>Wed, 15 Jan 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>The goal of Apache Hudi is to bring database-like features to data lakes. This addresses the main shortcoming of traditional data lakes: the inability to easily perform row-level updates or deletions.By integrating database-like management capabilities into data lakes, Hudi revolutionizes how it handles and processes large volumes of data, enabling out-of-the-box upserts and deletes that facilitate efficient record level updating and deletion.
One of Hudi's key innovations is the ability for users to explicitly define a Record Key, similar to a unique key in traditional databases, along with a Partition Key that aligns with the data lake paradigm. These two keys make the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java" target="_blank" rel="noopener noreferrer">HoodieKey</a> that aligns with the data lake paradigm. These two keys make the HoodieKey which is similar to the primary key which uniquely defines each row. This enables hudi to do the upsert based on Hoodiekey. The upsert operation works by utilizing the HoodieKey to locate the exact file group where the data associated with that key resides.  When a new record is ingested into the Hudi table, the system first derives  the HoodieKey of the incoming record based on the unique key and partitioning schema configured. This key is used to determine which file group (a logical grouping of files) the record should be associated with which is usually achieved via an <a href="https://hudi.apache.org/docs/indexes" target="_blank" rel="noopener noreferrer">indexing</a> mechanism.
In this blog, we will explore the concept of Key Generators in Apache Hudi, how they enhance data management, and their role in enabling efficient data operations in modern data lakes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="challenge">Challenge<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#challenge" class="hash-link" aria-label="Direct link to Challenge" title="Direct link to Challenge">​</a></h2>
<p>The biggest challenge in defining the record key and partition key on a table is  the columns in input data does not naturally lend itself to being used as a primary key or partition key directly. In the realm of databases, we often have below cases -</p>
<ul>
<li>Need to have multiple fields that serve as primary key commonly known as composite keys in the database.</li>
<li>It is necessary to preprocess the data to derive a specific field that can serve as a primary key before loading it into the database.</li>
<li>Sometimes we have to generate unique ids also. Common use case is surrogate key.</li>
</ul>
<p>Similarly, for partition columns also in datalakes, most of the time the raw field can’t be used as a partition key.</p>
<ul>
<li>Partition columns often have time grain like month level or year level partition but input data mostly contain timestamp and date.</li>
<li>Nested primary keys are very common, and necessitates multiple partition columns.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approaches-to-handling-this-in-data-pipelines">Approaches to Handling this in Data Pipelines<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#approaches-to-handling-this-in-data-pipelines" class="hash-link" aria-label="Direct link to Approaches to Handling this in Data Pipelines" title="Direct link to Approaches to Handling this in Data Pipelines">​</a></h2>
<p>Data Lake and Lakehouse technologies typically address such scenarios by preprocessing the data. For example, if date-based partitioning is required and a timestamp column is available, the data must be processed using Spark SQL date functions to extract relevant components (e.g., year, month, day). These derived columns are then used for partitioning. However, this process can become cumbersome at scale, especially when multiple data streams are writing to the same Hudi table. The same extraction logic needs to be applied to all streams, and any table maintenance activities (such as bootstrapping or backfilling) also require this logic to be reapplied. This repetition is error-prone and can lead to data consistency issues if the logic is incorrectly applied.
Hudi addresses these challenges with a built-in solution: key generators. These can be configured at the table level, eliminating the need to repeatedly apply the same logic. With key generators, Hudi automatically handles the conversion process every time, ensuring consistency and reducing the risk of errors.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-key-generators-in-apache-hudi">What are Key Generators in Apache Hudi<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#what-are-key-generators-in-apache-hudi" class="hash-link" aria-label="Direct link to What are Key Generators in Apache Hudi" title="Direct link to What are Key Generators in Apache Hudi">​</a></h2>
<p><a href="https://hudi.apache.org/docs/key_generation" target="_blank" rel="noopener noreferrer">Key generators</a> in Apache Hudi are essential components responsible for creating record keys and partition keys for records within a dataset. Hudi uses key generators to extract the Hudi record key, which is a combination of the record key and the partition key, from the incoming record fields. This process allows Hudi to efficiently prepare the hoodie key on which updates can occur. During upserts, Hudi identifies the file group that contains the specified hoodie key using an index and updates the corresponding file group accordingly.
Hudi offers several built-in key generator implementations that cover common use cases, such as generating record keys based on fields from the input data. However, to provide flexibility and support for more complex use cases, Hudi also offers a pluggable interface. This allows users to implement custom key generators tailored to their specific requirements.
To create a custom key generator, you can extend the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/BaseKeyGenerator.java" target="_blank" rel="noopener noreferrer">BaseKeyGenerator</a> class which itself extends the <a href="https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/keygen/KeyGenerator.java" target="_blank" rel="noopener noreferrer">KeyGenerator</a>  class and implement methods such as getRecordKey and getPartitionKey. This enables you to define the specific logic required for calculating record and partition keys tailored to your dataset's requirements. Additionally, Hudi includes a variety of built-in key generators that address many common scenarios discussed in the previous section, streamlining the process of key generation for users.
The key generator is configured at the table level and stored in the hoodie.properties file, which resides within the .hoodie directory. This file contains all the table-level configurations, including the key generation settings. Once a table is created with a particular key generator we can’t change it. It can be set using the configuration hoodie.datasource.write.keygenerator.class</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="out-of-the-box-key-generators">Out of the Box Key Generators<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#out-of-the-box-key-generators" class="hash-link" aria-label="Direct link to Out of the Box Key Generators" title="Direct link to Out of the Box Key Generators">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="simplekeygenerator">SimpleKeyGenerator<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#simplekeygenerator" class="hash-link" aria-label="Direct link to SimpleKeyGenerator" title="Direct link to SimpleKeyGenerator">​</a></h3>
<p>The SimpleKeyGenerator is a basic key generator used in Apache Hudi when direct fields from the input dataset can serve as both the record key and partition key. It maps a specific column in the DataFrame to the record key and another column to the partition path. This widely-used generator interprets values as-is from the DataFrame and converts them to strings, making it ideal for straightforward data structures.
Please note that this is the default key generator for the partitioned datasets.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field": "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "date",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.SimpleKeyGenerator"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nonpartitionedkeygenerator">NonpartitionedKeyGenerator<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#nonpartitionedkeygenerator" class="hash-link" aria-label="Direct link to NonpartitionedKeyGenerator" title="Direct link to NonpartitionedKeyGenerator">​</a></h3>
<p>The NonpartitionedKeyGenerator is a key generator in Apache Hudi designed specifically for non-partitioned datasets. Unlike the SimpleKeyGenerator, which uses a field to determine the partition path for the data, the NonpartitionedKeyGenerator does not assign a partition key to the records. Instead, it returns an empty string as the partition key for all records. This is because the dataset is non-partitioned, meaning all records are stored in a single partition.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field": "id",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class": "org.apache.hudi.keygen.NonpartitionedKeyGenerator"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="complexkeygenerator">ComplexKeyGenerator<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#complexkeygenerator" class="hash-link" aria-label="Direct link to ComplexKeyGenerator" title="Direct link to ComplexKeyGenerator">​</a></h3>
<p>This key generator is used when multiple fields are used to create the record key or partition key. We can provide the comma separated list of the columns. In the output, the hoodie record key is generated using the format key1<!-- -->:value1<!-- -->,key2<!-- -->:value2<!-- -->. If any one of the partition key or record key contains multiple fields, then we have to use ComplexKeyGenerator.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class" : "org.apache.hudi.keygen.ComplexKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.recordkey.field" = "key1,key2",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field" = "country,state,city"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="timestampbasedkeygenerator">TimestampBasedKeygenerator<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#timestampbasedkeygenerator" class="hash-link" aria-label="Direct link to TimestampBasedKeygenerator" title="Direct link to TimestampBasedKeygenerator">​</a></h3>
<p>The TimestampBasedKeyGenerator allows you to generate partition keys based on timestamp fields in your data. This is especially useful when you want to partition your data by date, month, or year, depending on your use case. The key generator can transform timestamps into different formats, enabling you to create partitions that suit your analytical needs.</p>
<p>Relevant Configurations</p>
<ul>
<li>
<p><strong>hoodie.datasource.write.keygenerator.class</strong>
To use this key generator, The key gen class should be <code>org.apache.hudi.keygen.TimestampBasedKeyGenerator</code></p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.timestamp.type</strong>
This config determines the nature of the value of input. Below can be the possible values for this -
<strong>DATE_STRING</strong>: Use this when the input value is in string format.</p>
<ul>
<li>
<p>MIXED: This option allows for a combination of formats.</p>
</li>
<li>
<p>UNIX_TIMESTAMP: Select this when the input value is in epoch timestamp format (long type) measured in seconds.</p>
</li>
<li>
<p>EPOCHMILLISECONDS: Use this when the input value is in epoch timestamp format (long type) measured in milliseconds.</p>
</li>
<li>
<p>SCALAR: This option is for epoch timestamp values (long type) where you can specify any time unit.</p>
</li>
</ul>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.timestamp.scalar.time.unit</strong>
When using the SCALAR timestamp type, you can define the unit of the epoch time. Valid options include NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS</p>
</li>
<li>
<p><strong>hoodie.keygen.timebased.input.dateformat</strong>
When the timestamp type is DATE_STRING or MIXED, this config can be defined to specify the date format in which the field is coming in input.</p>
</li>
<li>
<p><strong>hoodie.keygen.timebased.output.dateformat</strong>
When the timestamp type is set to DATE_STRING or MIXED, this configuration defines the desired date format for the output field. It allows you to specify how the date should be formatted when it is generated or output.</p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.input.timezone</strong>
This setting specifies the timezone for the input date field derived from the raw data. The default value is UTC.</p>
</li>
<li>
<p><strong>hoodie.deltastreamer.keygen.timebased.output.timezone</strong>
This setting defines the timezone for the output date field that will be used to populate the partition column. The default value is UTC.</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="common-use-cases">Common Use Cases<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#common-use-cases" class="hash-link" aria-label="Direct link to Common Use Cases" title="Direct link to Common Use Cases">​</a></h4>
<ul>
<li>Data Contains Timestamp Field and We Want Date Level Partitions
In this scenario, you have a dataset with a timestamp field, and you want to partition the data by the date (i.e., year-month-day).</li>
</ul>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "event_time"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>Data Contains Date Field but We Want to Have Month or Year Level Partitions
Here, you have a dataset with a date field, but you want to create partitions at a higher granularity, such as by month or year.</li>
</ul>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "event_date"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In the example above, if we have an input with a date column named event_date in the format 'yyyy-MM-dd', the configurations will convert this format to a monthly level in the format 'yyyyMM' and use it as the partition column.</p>
<p>We can refer <a href="https://hudi.apache.org/docs/0.10.0/key_generation/#timestampbasedkeygenerator" target="_blank" rel="noopener noreferrer">TimestampBasedKeyGenerator</a> for more examples</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="customkeygenerator">CustomKeyGenerator<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#customkeygenerator" class="hash-link" aria-label="Direct link to CustomKeyGenerator" title="Direct link to CustomKeyGenerator">​</a></h3>
<p>In typical use cases, using the same key generator for both the record key and the partition key often does not meet the requirements. For such scenarios, a Custom Key Generator is particularly useful, as it allows for the use of different key generators for different fields.
A common use case arises when the partition key consists of multiple fields, and you also need to extract date or month-level partitions from a timestamp field. In these situations, it is essential to utilize both the TimestampBasedKeyGenerator and the ComplexKeyGenerator. However, since you cannot specify two different key generator classes simultaneously, the CustomKeyGenerator serves as an effective solution. We can configure it as list of comma separated fields with the key generator separated by colon. Example - key1<!-- -->:Timestamp<!-- -->,key2<!-- -->:SIMPLE<!-- -->,key3<!-- -->:SIMPLE<!-- -->
When we pass the partition column, we can also provide which key generator to use. The configurations below enable you to use SimpleKeyGenerator to extract the country field and TimestampBasedKeygenerator to transform the event_date field to use only month level partitions.</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.keygenerator.class":     "org.apache.hudi.keygen.TimestampBasedKeyGenerator",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.deltastreamer.keygen.timebased.timestamp.type": "DATE_STRING",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.input.dateformat":"yyyy-MM-dd",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.keygen.timebased.output.dateformat":"yyyyMM",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  "hoodie.datasource.write.partitionpath.field": "country:SIMPLE,event_date:TIMESTAMP"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/blog/2025/01/15/outofbox-key-generators-in-hudi#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Key generators in Hudi are vital components that enable efficient record identification, partitioning, and data operations in large datasets. Whether you're performing upserts, deletes, or managing time-series data, choosing the right key generator ensures that Hudi can handle the data efficiently, while aligning with your business logic. By addressing challenges like composite keys, timestamp-based partitioning, and complex use cases, Apache Hudi revolutionizes how data lakes handle evolving data, providing database-like management capabilities that are scalable and flexible.</p>]]></content:encoded>
            <category>Data Lake</category>
            <category>Data Lakehouse</category>
            <category>Apache Hudi</category>
            <category>Key Generators</category>
            <category>partition</category>
        </item>
        <item>
            <title><![CDATA[Apache Hudi 2024: A Year In Review]]></title>
            <link>https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review</link>
            <guid>https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review</guid>
            <pubDate>Sun, 29 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi's ecosystem.]]></description>
            <content:encoded><![CDATA[<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/cover.jpg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<p>As we wrap up another remarkable year for Apache Hudi, I am thrilled to reflect on the tremendous achievements and milestones that have defined 2024. This year has been particularly special as we achieved several significant milestones, including the landmark release of Hudi 1.0, the publication of comprehensive books, and the introduction of new tools that expand Hudi's ecosystem.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-growth-and-engagement">Community Growth and Engagement<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-growth-and-engagement" class="hash-link" aria-label="Direct link to Community Growth and Engagement" title="Direct link to Community Growth and Engagement">​</a></h2>
<p>The Apache Hudi community continued its impressive growth trajectory in 2024. The number of new PRs has remained stable, indicating a consistent level of development activities:</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/pr-history.svg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<p>Our community presence expanded significantly across various platforms:</p>
<ul>
<li>The community grew to over 10,500 followers on LinkedIn</li>
<li>Added 8,755 new followers in the last 365 days</li>
<li>Generated 441,402 content impressions</li>
<li>Received 6,555 reactions and 493 comments across platforms</li>
<li>Our Slack community remained vibrant with rich technical discussions and knowledge sharing</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-milestones">Major Milestones<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#major-milestones" class="hash-link" aria-label="Direct link to Major Milestones" title="Direct link to Major Milestones">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-hudi-10-release">Apache Hudi 1.0 Release<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#apache-hudi-10-release" class="hash-link" aria-label="Direct link to Apache Hudi 1.0 Release" title="Direct link to Apache Hudi 1.0 Release">​</a></h3>
<p>2024 marked a historic moment with the <a href="https://hudi.apache.org/releases/release-1.0.0" target="_blank" rel="noopener noreferrer">release of Apache Hudi 1.0</a>, representing a major evolution in data lakehouse technology. This release brought several groundbreaking features:</p>
<ul>
<li><strong>Secondary Indexing</strong>: First of its kind in lakehouses, enabling database-like query acceleration with demonstrated 95% latency reduction on 10TB TPC-DS for low-moderate selectivity queries</li>
<li><strong>Logical Partitioning via Expression Indexes</strong>: Introducing PostgreSQL-style expression indexes for more efficient partition management</li>
<li><strong>Partial Updates</strong>: Achieving 2.6x performance improvement and 85% reduction in bytes written for update-heavy workloads</li>
<li><strong>Non-blocking Concurrency Control (NBCC)</strong>: An industry-first feature allowing simultaneous writing from multiple writers</li>
<li><strong>Merge Modes</strong>: First-class support for both <code>commit_time_ordering</code> and <code>event_time_ordering</code></li>
<li><strong>LSM Timeline</strong>: Revamped timeline storage as a scalable LSM tree for extended table history retention</li>
<li><strong>TrueTime</strong>: Strengthened time semantics ensuring forward-moving clocks in distributed processes</li>
</ul>
<p>Please check out the <a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0">announcement blog</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="launch-of-hudi-rs">Launch of Hudi-rs<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#launch-of-hudi-rs" class="hash-link" aria-label="Direct link to Launch of Hudi-rs" title="Direct link to Launch of Hudi-rs">​</a></h3>
<p>A significant expansion of the Hudi ecosystem occurred with the <a href="https://github.com/apache/hudi-rs" target="_blank" rel="noopener noreferrer">release of Hudi-rs</a>, the native Rust implementation for Apache Hudi with Python API bindings. This new project enables:</p>
<ul>
<li>Reading Hudi Tables without Spark or JVM dependencies</li>
<li>Integration with Apache Arrow for enhanced compatibility</li>
<li>Support for Copy-on-Write (CoW) table snapshots and time-travel reads</li>
<li>Cloud storage support across AWS, Azure, and GCP</li>
<li>Native integration with Apache DataFusion, Ray, Daft, etc</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="published-books-and-educational-content">Published Books and Educational Content<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#published-books-and-educational-content" class="hash-link" aria-label="Direct link to Published Books and Educational Content" title="Direct link to Published Books and Educational Content">​</a></h3>
<p>2024 saw the release of two comprehensive guides to Apache Hudi:</p>
<ul>
<li><a href="https://learning.oreilly.com/library/view/apache-hudi-the/9781098173821/" target="_blank" rel="noopener noreferrer"><strong>"Apache Hudi: The Definitive Guide"</strong></a> (O'Reilly) - Released in early access, <a href="https://www.onehouse.ai/whitepaper/apache-hudi-the-definitive-guide" target="_blank" rel="noopener noreferrer">free copy available</a>, providing comprehensive coverage of:<!-- -->
<ul>
<li>Distributed query engines</li>
<li>Snapshot and time travel queries</li>
<li>Incremental queries</li>
<li>Change-data-capture modes</li>
<li>End-to-end ingestion with Hudi Streamer</li>
</ul>
</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi-tdg.jpg" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li><a href="https://blog.datumagic.com/p/apache-hudi-from-zero-to-one-110" target="_blank" rel="noopener noreferrer"><strong>"Apache Hudi: From Zero to One"</strong></a> - A 10-part blog series turned into <a href="https://www.onehouse.ai/whitepaper/ebook-apache-hudi---zero-to-one" target="_blank" rel="noopener noreferrer">an ebook</a>, offering deep technical insights into Hudi's architecture and capabilities, covering:<!-- -->
<ul>
<li>Storage format and operations</li>
<li>Read and write flows</li>
<li>Table services and indexing</li>
<li>Incremental processing</li>
<li>Hudi 1.0 features</li>
</ul>
</li>
</ul>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/hudi0to1.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-events-and-sharing">Community Events and Sharing<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-events-and-sharing" class="hash-link" aria-label="Direct link to Community Events and Sharing" title="Direct link to Community Events and Sharing">​</a></h2>
<p>The Apache Hudi community maintained a strong presence at major industry events throughout 2024:</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/community-events.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li>Databricks' Data+AI Summit - Presenting Apache Hudi's role in the lakehouse ecosystem and its interoperability with other table formats through XTable, an open-source project enabling seamless conversion between Hudi, Delta Lake, and Iceberg</li>
<li>Confluent's Current 2024 - Demonstrating Hudi's powerful CDC capabilities with Apache Flink, showcasing real-time data pipelines and the innovative Non-Blocking Concurrency Control (NBCC) for high-volume streaming workloads</li>
<li>Trino Fest 2024 - Showcasing Hudi connector's evolution and innovations in Trino, including multi-modal indexing capabilities and the roadmap for enhanced query performance through Alluxio-powered caching and expanded DDL/DML support</li>
<li>Bangalore Lakehouse Days - Deep dive into Apache Hudi 1.0's groundbreaking features including LSM-based timeline, functional indexes, and non-blocking concurrency control, demonstrating Hudi's continued innovation in the lakehouse space</li>
</ul>
<p>Additionally, the community launched several new initiatives to foster learning and knowledge sharing:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lakehouse-chronicles-with-apache-hudi"><a href="https://www.youtube.com/playlist?list=PLxSSOLH2WRMNQetyPU98B2dHnYv91R6Y8" target="_blank" rel="noopener noreferrer">Lakehouse Chronicles with Apache Hudi</a><a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#lakehouse-chronicles-with-apache-hudi" class="hash-link" aria-label="Direct link to lakehouse-chronicles-with-apache-hudi" title="Direct link to lakehouse-chronicles-with-apache-hudi">​</a></h3>
<p>A new community series with 4 episodes released.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/lakehouse-chronicles.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hudi-newsletter"><a href="https://hudinewsletter.substack.com/" target="_blank" rel="noopener noreferrer">Hudi Newsletter</a><a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#hudi-newsletter" class="hash-link" aria-label="Direct link to hudi-newsletter" title="Direct link to hudi-newsletter">​</a></h3>
<p>9 editions published, keeping the community informed about latest developments.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/newsletter.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="community-syncs"><a href="https://www.youtube.com/@apachehudi" target="_blank" rel="noopener noreferrer">Community Syncs</a><a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#community-syncs" class="hash-link" aria-label="Direct link to community-syncs" title="Direct link to community-syncs">​</a></h3>
<p>Featured 8 user stories from major organizations including Amazon, Peloton, Shopee and Uber.</p>
<img src="https://hudi.apache.org/assets/images/blog/2024-12-29-a-year-in-review-2024/community-syncs.png" alt="drawing" style="width:80%;display:block;margin-left:auto;margin-right:auto;margin-top:18pt;margin-bottom:18pt">
<ul>
<li><a href="https://www.youtube.com/watch?v=rMXhlb7Uci8" target="_blank" rel="noopener noreferrer">Powering Amazon Unit Economics with Configurations and Hudi</a></li>
<li><a href="https://www.youtube.com/watch?v=-Pyid5K9dyU" target="_blank" rel="noopener noreferrer">Modernizing Data Infrastructure at Peleton using Apache Hudi</a></li>
<li><a href="https://www.youtube.com/watch?v=fqhr-4jXi6I" target="_blank" rel="noopener noreferrer">Innovative Solution for Real-time Analytics at Scale using Apache Hudi (Shopee)</a></li>
<li><a href="https://www.youtube.com/watch?v=VpdimpH_nsI" target="_blank" rel="noopener noreferrer">Scaling Complex Data Workflows using Apache Hudi (Uber)</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="notable-user-stories-and-technical-content">Notable User Stories and Technical Content<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#notable-user-stories-and-technical-content" class="hash-link" aria-label="Direct link to Notable User Stories and Technical Content" title="Direct link to Notable User Stories and Technical Content">​</a></h2>
<p>Throughout 2024, several organizations shared their Hudi implementation experiences:</p>
<ul>
<li><a href="https://www.notion.com/blog/building-and-scaling-notions-data-lake" target="_blank" rel="noopener noreferrer">Notion's transition from Snowflake to Hudi</a></li>
<li><a href="https://engineering.grab.com/enabling-near-realtime-data-analytics" target="_blank" rel="noopener noreferrer">Grab's implementation of near-realtime data analytics</a></li>
<li><a href="https://aws.amazon.com/blogs/big-data/use-aws-data-exchange-to-seamlessly-share-apache-hudi-datasets/" target="_blank" rel="noopener noreferrer">AWS's data sharing capabilities with AWS Data Exchange</a></li>
<li><a href="https://www.y.uno/post/how-apache-hudi-transformed-yunos-data-lake" target="_blank" rel="noopener noreferrer">Yuno's data lake transformation</a></li>
<li><a href="https://blogs.halodoc.io/data-lake-cost-optimisation-strategies/" target="_blank" rel="noopener noreferrer">Halodoc's cost optimization strategies</a></li>
<li><a href="https://medium.com/upstox-engineering/navigating-the-future-the-evolutionary-journey-of-upstoxs-data-platform-92dc10ff22ae" target="_blank" rel="noopener noreferrer">Upstox's data platform evolution</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="looking-ahead-to-2025">Looking Ahead to 2025<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#looking-ahead-to-2025" class="hash-link" aria-label="Direct link to Looking Ahead to 2025" title="Direct link to Looking Ahead to 2025">​</a></h2>
<p>As we look forward to 2025, Apache Hudi's roadmap includes several exciting developments:</p>
<ul>
<li>Enhanced core engine with modernized write paths and advanced indexing (bitmap, vector search)</li>
<li>Multi-modal data support with improved storage engine APIs and cross-format interoperability</li>
<li>Enterprise-grade features including multi-table transactions and advanced caching</li>
<li>Robust platform services with Data Lakehouse Management System (DLMS) components</li>
<li>Broader adoption of Hudi-rs across the ecosystem</li>
<li>Continued focus on stability and seamless migration path for the community</li>
</ul>
<p>These initiatives reflect our commitment to advancing data lakehouse technology while ensuring reliability and user experience.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-involved">Get Involved<a href="https://hudi.apache.org/blog/2024/12/29/apache-hudi-2024-a-year-in-review#get-involved" class="hash-link" aria-label="Direct link to Get Involved" title="Direct link to Get Involved">​</a></h2>
<p>Join our thriving community:</p>
<ul>
<li>Contribute to the project on GitHub: <a href="https://github.com/apache/hudi" target="_blank" rel="noopener noreferrer">Hudi</a> &amp; <a href="https://github.com/apache/hudi-rs" target="_blank" rel="noopener noreferrer">Hudi-rs</a></li>
<li>Join our <a href="https://apache-hudi.slack.com/join/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g" target="_blank" rel="noopener noreferrer">Slack community</a></li>
<li>Follow us on <a href="https://www.linkedin.com/company/apache-hudi/" target="_blank" rel="noopener noreferrer">LinkedIn</a> and <a href="https://x.com/apachehudi" target="_blank" rel="noopener noreferrer">X (Twitter)</a></li>
<li>Subscribe to our <a href="https://www.youtube.com/@apachehudi" target="_blank" rel="noopener noreferrer">YouTube channel</a></li>
<li>Participate in our <a href="https://hudi.apache.org/community/syncs" target="_blank" rel="noopener noreferrer">community syncs</a> and <a href="https://hudi.apache.org/community/office_hours" target="_blank" rel="noopener noreferrer">office hours</a>.</li>
<li>Subscribe to the dev mailing list by sending an empty email to <code>dev-subscribe@hudi.apache.org</code></li>
</ul>
<p>The success of Apache Hudi in 2024 wouldn't have been possible without our dedicated community of contributors, users, and supporters. As we celebrate these achievements, we look forward to another year of innovation and growth in 2025.</p>]]></content:encoded>
            <category>apache hudi</category>
            <category>community</category>
        </item>
        <item>
            <title><![CDATA[Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses]]></title>
            <link>https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0</link>
            <guid>https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0</guid>
            <pubDate>Mon, 16 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Overview]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>We are thrilled to announce the release of Apache Hudi 1.0, a landmark achievement for our vibrant community that defines what the next generation of data lakehouses should achieve. Hudi pioneered <em><strong>transactional data lakes</strong></em> in 2017, and today, we live in a world where this technology category is mainstream as the “<em><strong>Data Lakehouse”</strong></em>. The Hudi community has made several key, original, and first-of-its-kind contributions to this category, as shown below, compared to when other OSS alternatives emerged. This is an incredibly rare feat for a relatively small OSS community to sustain in a fiercely competitive commercial data ecosystem. On the other hand, it also demonstrates the value of deeply understanding the technology category within a focused open-source community. So, I first want to thank/congratulate the Hudi community and the <strong>60+ contributors</strong> for making 1.0 happen.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/blog/hudi-innovation-timeline.jpg" alt="innovation timeline"></div>
<p>This <a href="https://hudi.apache.org/releases/release-1.0.0">release</a> is more than just a version increment—it advances the breadth of Hudi’s feature set and its architecture's robustness while bringing fresh innovation to shape the future. This post reflects on how technology and the surrounding ecosystem have evolved, making a case for a holistic “<em><strong>Data Lakehouse Management System</strong></em>” (<em><strong>DLMS</strong></em>) as the new Northstar. For most of this post, we will deep dive into the latest capabilities of Hudi 1.0 that make this evolution possible.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="evolution-of-the-data-lakehouse">Evolution of the Data Lakehouse<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#evolution-of-the-data-lakehouse" class="hash-link" aria-label="Direct link to Evolution of the Data Lakehouse" title="Direct link to Evolution of the Data Lakehouse">​</a></h2>
<p>Technologies must constantly evolve—<a href="https://en.wikipedia.org/wiki/Web3" target="_blank" rel="noopener noreferrer">Web 3.0</a>, <a href="https://en.wikipedia.org/wiki/List_of_wireless_network_technologies" target="_blank" rel="noopener noreferrer">cellular tech</a>, <a href="https://en.wikipedia.org/wiki/Programming_language_generations" target="_blank" rel="noopener noreferrer">programming language generations</a>—based on emerging needs. Data lakehouses are no exception. This section explores the hierarchy of such needs for data lakehouse users. The most basic need is the “<strong>table format</strong>” functionality, the foundation for data lakehouses. Table format organizes the collection of files/objects into tables with snapshots, schema, and statistics tracking, enabling higher abstraction. Furthermore, table format dictates the organization of files within each snapshot, encoding deletes/updates and metadata about how the table changes over time. Table format also provides protocols for various readers and writers and table management processes to handle concurrent access and provide ACID transactions safely. In the last five years, leading data warehouse and cloud vendors have integrated their proprietary SQL warehouse stack with open table formats. While they mostly default to their closed table formats and the compute engines remain closed, this welcome move provides users an open alternative for their data.</p>
<p>However, the benefits of a format end there, and now a table format is just the tip of the iceberg. Users require an <a href="https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective" target="_blank" rel="noopener noreferrer">end-to-end open data lakehouse</a>, and modern data lakehouse features need a sophisticated layer of <em><strong>open-source software</strong></em> operating on data stored in open table formats. For example, Optimized writers can balance cost and performance by carefully managing file sizes using the statistics maintained in the table format or catalog syncing service that can make data in Hudi readily available to half a dozen catalogs open and closed out there. Hudi shines by providing a high-performance open table format as well as a comprehensive open-source software stack that can ingest, store, optimize and effectively self-manage a data lakehouse. This distinction between open formats and open software is often lost in translation inside the large vendor ecosystem in which Hudi operates. Still, it has been and remains a key consideration for Hudi’s <a href="https://hudi.apache.org/powered-by">users</a> to avoid compute-lockin to any given data vendor. The Hudi streamer tool, e.g., powers hundreds of data lakes by ingesting data seamlessly from various sources at the convenience of a single command in a terminal.</p>
<div style="text-align:center;width:90%;height:auto"><img src="https://hudi.apache.org/assets/images/blog/dlms-hierarchy.png" alt="dlms hierarchy"></div>
<p>Moving forward with 1.0, the community has <a href="https://github.com/apache/hudi/pull/8679" target="_blank" rel="noopener noreferrer">debated</a> these key points and concluded that we need more open-source “<strong>software capabilities</strong>” that are directly comparable with DBMSes for two main reasons.</p>
<p><strong>Significantly expand the technical capabilities of a data lakehouse</strong>: Many design decisions in Hudi have been inspired by databases (see <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md#hudi-1x" target="_blank" rel="noopener noreferrer">here</a> for a layer-by-layer mapping) and have delivered significant benefits to the community. For example, Hudi’s indexing mechanisms deliver the fast update performance the project has come to be known for.  We want to generalize such features across writers and queries and introduce new capabilities like fast metastores for query planning, support for unstructured/multimodal data and caching mechanisms that can be deeply integrated into (at least) open-source query engines in the ecosystem. We also need concurrency control that works for lakehouse workloads instead of employing techniques applicable to OLTP databases at the surface level.</p>
<p><strong>We also need a database-like experience</strong>: We originally designed Hudi as a software library that can be embedded into different query/processing engines for reading/writing/managing tables. This model has been a great success within the existing data ecosystem, which is familiar with scheduling jobs and employing multiple engines for ETL and interactive queries. However, for a new user wanting to explore data lakehouses, there is no piece of software to easily install and explore all functionality packaged coherently. Such data lakehouse functionality packaged and delivered like a typical database system unlocks new use cases. For example, with such a system, we could bring HTAP capabilities to the data lakehouses on faster cloud storage/row-oriented formats, finally making it a low-latency data serving layer.</p>
<p>If combined, we would gain a powerful database built on top of the data lake(house) architecture—a <em><strong>data</strong></em> <em><strong>lakehouse</strong></em> <em><strong>management</strong></em> <em><strong>system</strong></em> <em><strong>(DLMS)</strong></em>—that we believe the industry needs.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-features-in-hudi-10">Key Features in Hudi 1.0<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#key-features-in-hudi-10" class="hash-link" aria-label="Direct link to Key Features in Hudi 1.0" title="Direct link to Key Features in Hudi 1.0">​</a></h2>
<p>In Hudi 1.0, we’ve delivered a significant expansion of data lakehouse technical capabilities discussed above inside Hudi’s <a href="https://en.wikipedia.org/wiki/Database_engine" target="_blank" rel="noopener noreferrer">storage engine</a> layer.  Storage engines (a.k.a database engines) are standard database components that sit on top of the storage/file/table format and are wrapped by the DBMS layer above, handling the core read/write/management functionality. In the figure below, we map the Hudi components with the seminal <a href="https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf" target="_blank" rel="noopener noreferrer">Architecture of a Database System</a> paper (see page 4) to illustrate the standard layering discussed. If the layering is implemented correctly, we can deliver the benefits of the storage engine to even other table formats, which may lack such fully-developed open-source software for table management or achieving high performance, via interop standards defined in projects like <a href="https://xtable.apache.org/" target="_blank" rel="noopener noreferrer">Apache XTable (Incubating)</a>.</p>
<div style="text-align:center;width:80%;height:auto"><img src="https://hudi.apache.org/assets/images/hudi-stack-1-x.png" alt="Hudi DB Architecture"><p align="center">Figure: Apache Hudi Database Architecture</p></div>
<p>Regarding full-fledged DLMS functionality, the closest experience Hudi 1.0 offers is through Apache Spark. Users can deploy a Spark server (or Spark Connect) with Hudi 1.0 installed, submit SQL/jobs, orchestrate table services via SQL commands, and enjoy new secondary index functionality to speed up queries like a DBMS. Subsequent releases in the 1.x release line and beyond will continuously add new features and improve this experience.</p>
<p>In the following sections, let’s dive into what makes Hudi 1.0 a standout release.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-time-and-timeline">New Time and Timeline<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#new-time-and-timeline" class="hash-link" aria-label="Direct link to New Time and Timeline" title="Direct link to New Time and Timeline">​</a></h3>
<p>For the familiar user, time is a key concept in Hudi. Hudi’s original notion of time was instantaneous, i.e., actions that modify the table appear to take effect at a given instant. This was limiting when designing features like non-blocking concurrency control across writers, which needs to reason about actions more as an “interval” to detect other conflicting actions. Every action on the Hudi timeline now gets a <em>requested</em> and a <em>completion</em> time; Thus, the timeline layout version has bumped up in the 1.0 release. Furthermore, to ease the understanding and bring consistency around time generation for users and implementors, we have formalized the adoption of <a href="https://hudi.apache.org/docs/timeline#truetime-generation">TrueTime</a> semantics. The default implementation assures forward-moving clocks even with distributed processes, assuming a maximum tolerable clock skew similar to <a href="https://cockroachlabs.com/blog/living-without-atomic-clocks/" target="_blank" rel="noopener noreferrer">OLTP/NoSQL</a> stores adopting TrueTime.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/hudi-timeline-actions.png" alt="Timeline actions"><p align="center">Figure: Showing actions in Hudi 1.0 modeled as an interval of two instants: requested and completed</p></div>
<p>Hudi tables are frequently updated, and users also want to retain a more extended action history associated with the table. Before Hudi 1.0, the older action history in a table was archived for audit access. But, due to the lack of support for cloud storage appends, access might become cumbersome due to tons of small files. In Hudi 1.0, we have redesigned the timeline as an <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank" rel="noopener noreferrer">LSM tree</a>, which is widely adopted for cases where good write performance on temporal data is desired.</p>
<p>In the Hudi 1.0 release, the <a href="https://hudi.apache.org/docs/timeline#lsm-timeline-history">LSM timeline</a> is heavily used in the query planning to map requested and completion times across Apache Spark, Apache Flink and Apache Hive. Future releases plan to leverage this to unify the timeline's active and history components, providing infinite retention of table history. Micro benchmarks show that the LSM timeline can be pretty efficient, even committing every <em><strong>30 seconds for 10 years with about 10M instants</strong></em>, further cementing Hudi’s table format as the most suited for frequently written tables.</p>
<table><thead><tr><th style="text-align:left">Number of actions</th><th style="text-align:left">Instant Batch Size</th><th style="text-align:left">Read cost (just times)</th><th style="text-align:left">Read cost (along with action metadata)</th><th style="text-align:left">Total file size</th></tr></thead><tbody><tr><td style="text-align:left">10000</td><td style="text-align:left">10</td><td style="text-align:left">32ms</td><td style="text-align:left">150ms</td><td style="text-align:left">8.39MB</td></tr><tr><td style="text-align:left">20000</td><td style="text-align:left">10</td><td style="text-align:left">51ms</td><td style="text-align:left">188ms</td><td style="text-align:left">16.8MB</td></tr><tr><td style="text-align:left">10000000</td><td style="text-align:left">1000</td><td style="text-align:left">3400ms</td><td style="text-align:left">162s</td><td style="text-align:left">8.4GB</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="secondary-indexing-for-faster-lookups">Secondary Indexing for Faster Lookups<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#secondary-indexing-for-faster-lookups" class="hash-link" aria-label="Direct link to Secondary Indexing for Faster Lookups" title="Direct link to Secondary Indexing for Faster Lookups">​</a></h3>
<p>Indexes are core to Hudi’s design, so much so that even the first pre-open-source version of Hudi shipped with <a href="https://hudi.apache.org/docs/indexes#additional-writer-side-indexes">indexes</a> to speed up writes. However, these indexes were limited to the writer's side, except for record indexes in 0.14+ above, which were also integrated with Spark SQL queries. Hudi 1.0 generalizes indexes closer to the indexing functionality found in relational databases, supporting indexes on any secondary column across both writer and readers. Hudi 1.0 also supports near-standard <a href="https://hudi.apache.org/docs/sql_ddl#create-index">SQL syntax</a> for creating/dropping indexes on different columns via Spark SQL, along with an asynchronous indexing table service to build indexes without interrupting the writers.</p>
<div style="text-align:center;padding-left:10%;width:70%;height:auto"><img src="https://hudi.apache.org/assets/images/hudi-stack-indexes.png" alt="Indexes"><p align="center">Figure: the indexing subsystem in Hudi 1.0, showing different types of indexes</p></div>
<p>With secondary indexes, queries and DMLs scan a much-reduced amount of files from cloud storage, dramatically reducing costs (e.g., on engines like AWS Athena, which price by data scanned) and improving query performance for queries with low to even moderate amount of selectivity. On a benchmark of a query on <em>web_sales</em> table (from <em><strong>10 TB tpc-ds dataset</strong></em>), with file groups - 286,603, total records - 7,198,162,544 and cardinality of secondary index column in the ~ 1:150 ranges, we see a remarkable <em><strong>~95% decrease in latency</strong></em>.</p>
<table><thead><tr><th style="text-align:left">Run 1</th><th style="text-align:left">Total Query Latency w/o indexing skipping (secs)</th><th style="text-align:left">Total Query Latency with secondary index skipping (secs)</th><th style="text-align:left">% decrease</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left">252</td><td style="text-align:left">31</td><td style="text-align:left">~88%</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">214</td><td style="text-align:left">10</td><td style="text-align:left">~95%</td></tr><tr><td style="text-align:left">3</td><td style="text-align:left">204</td><td style="text-align:left">9</td><td style="text-align:left">~95%</td></tr></tbody></table>
<p>In Hudi 1.0, secondary indexes are only supported for Apache Spark, with planned support for other engines in Hudi 1.1, starting with Flink, Presto and Trino.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="bloom-filter-indexes">Bloom Filter indexes<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#bloom-filter-indexes" class="hash-link" aria-label="Direct link to Bloom Filter indexes" title="Direct link to Bloom Filter indexes">​</a></h3>
<p>Bloom filter indexes have existed on the Hudi writers for a long time. It is one of the most performant and versatile indexes users prefer for “needle-in-a-haystack” deletes/updates or de-duplication. The index works by storing special footers in base files around min/max key ranges and a dynamic bloom filter that adapts to the file size and can automatically handle partitioning/skew on the writer's path. Hudi 1.0 introduces a newer kind of bloom filter index for Spark SQL while retaining the writer-side index as-is. The new index stores bloom filters in the Hudi metadata table and other secondary/record indexes for scalable access, even for huge tables, since the index is stored in fewer files compared to being stored alongside data files. It can be created using standard <a href="https://hudi.apache.org/docs/sql_ddl#create-bloom-filter-index">SQL syntax</a>, as shown below. Subsequent queries on the indexed columns will use the bloom filters to speed up queries.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)">-- Create a bloom filter index on the driver column of the table `hudi_table`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_bloom_driver </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_indexed_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> bloom_filters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">driver</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- Create a bloom filter index on the column derived from expression `lower(rider)` of the table `hudi_table`</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">INDEX</span><span class="token plain"> idx_bloom_rider </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">ON</span><span class="token plain"> hudi_indexed_table </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">USING</span><span class="token plain"> bloom_filters</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">rider</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> OPTIONS</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">expr</span><span class="token operator">=</span><span class="token string" style="color:rgb(255, 121, 198)">'lower'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In future releases of Hudi, we aim to fully integrate the benefits of the older writer-side index into the new bloom index. Nonetheless, this demonstrates the adaptability of Hudi’s indexing system to handle different types of indexes on the table.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="partitioning-replaced-by-expression-indexes">Partitioning replaced by Expression Indexes<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#partitioning-replaced-by-expression-indexes" class="hash-link" aria-label="Direct link to Partitioning replaced by Expression Indexes" title="Direct link to Partitioning replaced by Expression Indexes">​</a></h3>
<p>An astute reader may have noticed above that the indexing is supported on a function/expression on a column. Hudi 1.0 introduces expression indexes similar to <a href="https://www.postgresql.org/docs/current/indexes-expressional.html" target="_blank" rel="noopener noreferrer">Postgres</a> to generalize a two-decade-old relic in the data lake ecosystem - partitioning! At a high level, partitioning on the data lake divides the table into folders based on a column or a mapping function (partitioning function). When queries or operations are performed against the table, they can efficiently skip entire partitions (folders), reducing the amount of metadata and data involved. This is very effective since data lake tables span 100s of thousands of files. But, as simple as it sounds, this is one of the <a href="https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse" target="_blank" rel="noopener noreferrer">most common pitfalls</a> around performance on the data lake, where new users use it like an index by partitioning based on a high cardinality column, resulting in lots of storage partitions/tiny files and abysmal write/query performance for no good reason. Further, tying storage organization to partitioning makes it inflexible to changes.</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/expression-index-date-partitioning.png" alt="Timeline actions"><p align="center">Figure: Shows index on a date expression when a different column physically partitions data</p></div>
<p>Hudi 1.0 treats partitions as a <a href="https://hudi.apache.org/docs/sql_queries#query-using-column-stats-expression-index">coarse-grained index</a> on a column value or an expression of a column, as they should have been. To support the efficiency of skipping entire storage paths/folders, Hudi 1.0 introduces partition stats indexes that aggregate these statistics on the storage partition path level, in addition to doing so at the file level. Now, users can create different types of indexes on columns to achieve the effects of partitioning in a streamlined fashion using fewer concepts to achieve the same results. Along with support for other 1.x features, partition stats and expression indexes support will be extended to other engines like Presto, Trino, Apache Doris, and Starrocks with the 1.1 release.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="efficient-partial-updates">Efficient Partial Updates<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#efficient-partial-updates" class="hash-link" aria-label="Direct link to Efficient Partial Updates" title="Direct link to Efficient Partial Updates">​</a></h3>
<p>Managing large-scale datasets often involves making fine-grained changes to records. Hudi has long supported <a href="https://hudi.apache.org/docs/0.15.0/record_payload#partialupdateavropayload">partial updates</a> to records via the record payload interface. However, this usually comes at the cost of sacrificing engine-native performance by moving away from specific objects used by engines to represent rows. As users have embraced Hudi for incremental SQL pipelines on top of dbt/Spark or Flink Dynamic Tables, there was a rise in interest in making this much more straightforward and mainstream. Hudi 1.0 introduces first-class support for <strong>partial updates</strong> at the log format level, enabling <em>MERGE INTO</em> SQL statements to modify only the changed fields of a record instead of rewriting/reprocessing the entire row.</p>
<p>Partial updates improve query and write performance simultaneously by reducing write amplification for writes and the amount of data read by Merge-on-Read snapshot queries. It also achieves much better storage utilization due to fewer bytes stored and improved compute efficiency over existing partial update support by retaining vectorized engine-native processing. Using the 1TB Brooklyn benchmark for write performance, we observe about <strong>2.6x</strong> improvement in Merge-on-Read query performance due to an <strong>85%</strong> reduction in write amplification. For random write workloads, the gains can be much more pronounced. Below shows a second benchmark for partial updates, 1TB MOR table, 1000 partitions, 80% random updates. 3/100 columns randomly updated.</p>
<table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Full Record Update</th><th style="text-align:left">Partial Update</th><th style="text-align:left">Gains</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Update latency (s)</strong></td><td style="text-align:left">2072</td><td style="text-align:left">1429</td><td style="text-align:left">1.4x</td></tr><tr><td style="text-align:left"><strong>Bytes written (GB)</strong></td><td style="text-align:left">891.7</td><td style="text-align:left">12.7</td><td style="text-align:left">70.2x</td></tr><tr><td style="text-align:left"><strong>Query latency (s)</strong></td><td style="text-align:left">164</td><td style="text-align:left">29</td><td style="text-align:left">5.7x</td></tr></tbody></table>
<p>This also lays the foundation for managing unstructured and multimodal data inside a Hudi table and supporting <a href="https://github.com/apache/hudi/pull/11733" target="_blank" rel="noopener noreferrer">wide tables</a> efficiently for machine learning use cases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="merge-modes-and-custom-mergers">Merge Modes and Custom Mergers<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#merge-modes-and-custom-mergers" class="hash-link" aria-label="Direct link to Merge Modes and Custom Mergers" title="Direct link to Merge Modes and Custom Mergers">​</a></h3>
<p>One of the most unique capabilities Hudi provides is how it helps process streaming data. Specifically, Hudi has, since the very beginning, supported merging records pre-write (to reduce write amplification), during write (against an existing record in storage with the same record key) and reads (for MoR snapshot queries), using a <em>precombine</em> or <em>ordering</em> field. This helps implement <a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/" target="_blank" rel="noopener noreferrer">event time processing</a> semantics, widely supported by stream processing systems, on data lakehouse storage. This helps integrate late-arriving data into Hudi tables without causing weird movement of record state back in time. For example, if an older database CDC record arrives late and gets committed as the new value, the state of the record would be incorrect even though the writes to the table themselves were serialized in some order.</p>
<p><img decoding="async" loading="lazy" alt="event time ordering" src="https://hudi.apache.org/assets/images/event-time-ordering-merge-mode-c8164e035840388bf4290fa81ac6262a.png" width="1360" height="490" class="img_ev3q">
</p><p align="center">Figure: Shows EVENT_TIME_ORDERING where merging reconciles state based on the highest event_time</p><p></p>
<p>Prior Hudi versions supported this functionality through the record payload interface with built-in support for a pre-combine field on the default payloads. Hudi 1.0 makes these two styles of processing and merging changes first class by introducing <a href="https://hudi.apache.org/docs/record_merger">merge modes</a> within Hudi.</p>
<table><thead><tr><th style="text-align:left">Merge Mode</th><th style="text-align:left">What does it do?</th></tr></thead><tbody><tr><td style="text-align:left">COMMIT_TIME_ORDERING</td><td style="text-align:left">Picks record with highest completion time/instant as final merge result  i.e., standard relational semantics or arrival time processing</td></tr><tr><td style="text-align:left">EVENT_TIME_ORDERING</td><td style="text-align:left">Default (for now, to ease migration).Picks record with the highest value for a user-specified ordering/precombine field as the final merge result.</td></tr><tr><td style="text-align:left">CUSTOM</td><td style="text-align:left">Uses a user-provided RecordMerger implementation to produce final merge result (similar to stream processing processor APIs)</td></tr></tbody></table>
<p>Like partial update support, the new <em>RecordMerger</em> API provides a more efficient engine-native alternative to the older RecordPayload interface through native objects and vectorized processing on EVENT_TIME_ORDERING merge modes. In future versions, we intend to change the default to COMMIT_TIME_ORDERING to provide simple, out-of-the-box relational table semantics.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="non-blocking-concurrency-control-for-streaming-writes">Non-Blocking Concurrency Control for Streaming Writes<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#non-blocking-concurrency-control-for-streaming-writes" class="hash-link" aria-label="Direct link to Non-Blocking Concurrency Control for Streaming Writes" title="Direct link to Non-Blocking Concurrency Control for Streaming Writes">​</a></h3>
<p>We have expressed dissatisfaction with the optimistic concurrency control approaches employed on the data lakehouse since they appear to paint the problem with a broad brush without paying attention to the nuances of the lakehouse workloads. Specifically, contention is much more common in data lakehouses, even for Hudi, the only data lakehouse storage project capable of asynchronously compacting delta updates without failing or causing retries on the writer. Ultimately, data lakehouses are high-throughput systems, and failing concurrent writers to handle contention can waste expensive compute clusters. Streaming and high-frequency writes often require fine-grained concurrency control to prevent bottlenecks.</p>
<p>Hudi 1.0 introduces a new <strong>non-blocking concurrency control (NBCC)</strong> designed explicitly for data lakehouse workloads, using years of experience gained supporting some of the largest data lakes on the planet in the Hudi community. NBCC enables simultaneous writing from multiple writers and compaction of the same record without blocking any involved processes. This is achieved by simply lightweight distributed locks and TrueTime semantics discussed above. (see <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md" target="_blank" rel="noopener noreferrer">RFC-66</a> for more)</p>
<div style="text-align:center"><img src="https://hudi.apache.org/assets/images/nbcc_partial_updates.gif" alt="NBCC"><p align="center">Figure: Two streaming jobs in action writing to the same records concurrently on different columns.</p></div>
<p>NBCC operates with streaming semantics, tying together concepts from previous sections. Data necessary to compute table updates are emitted from an upstream source, and changes and partial updates can be merged in any of the merge modes above. For example, in the figure above, two independent Flink jobs enrich different table columns in parallel, a pervasive pattern seen in stream processing use cases. Check out this <a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control" target="_blank" rel="noopener noreferrer">blog</a> for a full demo. We also expect to support NBCC across other compute engines in future releases.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="backwards-compatible-writing">Backwards Compatible Writing<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#backwards-compatible-writing" class="hash-link" aria-label="Direct link to Backwards Compatible Writing" title="Direct link to Backwards Compatible Writing">​</a></h3>
<p>If you are wondering: “All of this sounds cool, but how do I upgrade?” we have put a lot of thought into making that seamless. Hudi has always supported backward-compatible reads to older table versions. Table versions are stored in table properties unrelated to the software binary version. The supported way of upgrading has been to first migrate readers/query engines to new software binary versions and then upgrade the writers, which will auto-upgrade the table if there is a table version change between the old and new software binary versions. Upon community feedback, users expressed the need to be able to do upgrades on the writers without waiting on the reader side upgrades and reduce any additional coordination necessary within different teams.</p>
<p><img decoding="async" loading="lazy" alt="Indexes" src="https://hudi.apache.org/assets/images/backwards-compat-writing-6299b055646e2577964069b755ee1f3d.png" width="1481" height="825" class="img_ev3q">
</p><p align="center">Figure: 4-step process for painless rolling upgrades to Hudi 1.0</p><p></p>
<p>Hudi 1.0 introduces backward-compatible writing to achieve this in 4 steps, as described above. Hudi 1.0 also automatically handles any checkpoint translation necessary as we switch to completion time-based processing semantics for incremental and CDC queries. The Hudi metadata table has to be temporarily disabled during this upgrade process but can be turned on once the upgrade is completed successfully. Please read the <a href="https://hudi.apache.org/releases/release-1.0.0">release notes</a> carefully to plan your migration.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="whats-next">What’s Next?<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#whats-next" class="hash-link" aria-label="Direct link to What’s Next?" title="Direct link to What’s Next?">​</a></h2>
<p>Hudi 1.0 is a testament to the power of open-source collaboration. This release embodies the contributions of 60+ developers, maintainers, and users who have actively shaped its roadmap. We sincerely thank the Apache Hudi community for their passion, feedback, and unwavering support.</p>
<p>The release of Hudi 1.0 is just the beginning. Our current <a href="https://hudi.apache.org/roadmap">roadmap</a> includes exciting developments across the following planned releases:</p>
<ul>
<li><strong>1.0.1</strong>: First bug fix, patch release on top of 1.0, which hardens the functionality above and makes it easier. We intend to publish additional patch releases to aid migration to 1.0 as the bridge release for the community from 0.x.</li>
<li><strong>1.1</strong>:  Faster writer code path rewrite, new indexes like bitmap/vector search, granular record-level change encoding, Hudi storage engine APIs, abstractions for cross-format interop.</li>
<li><strong>1.2</strong>: Multi-table transactions, platform services for reverse streaming from Hudi etc., Multi-modal data + indexing, NBCC clustering</li>
<li><strong>2.0</strong>: Server components for DLMS, caching and metaserver functionality.</li>
</ul>
<p>Hudi releases are drafted collaboratively by the community. If you don’t see something you like here, please help shape the roadmap together.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="get-started-with-apache-hudi-10">Get Started with Apache Hudi 1.0<a href="https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#get-started-with-apache-hudi-10" class="hash-link" aria-label="Direct link to Get Started with Apache Hudi 1.0" title="Direct link to Get Started with Apache Hudi 1.0">​</a></h2>
<p>Are you ready to experience the future of data lakehouses? Here’s how you can dive into Hudi 1.0:</p>
<ul>
<li>Documentation: Explore Hudi’s <a href="https://hudi.apache.org/docs/overview">Documentation</a> and learn the <a href="https://hudi.apache.org/docs/hudi_stack">concepts</a>.</li>
<li>Quickstart Guide: Follow the <a href="https://hudi.apache.org/docs/quick-start-guide">Quickstart Guide</a> to set up your first Hudi project.</li>
<li>Upgrading from a previous version?  Follow the <a href="https://hudi.apache.org/releases/release-1.0.0#migration-guide">migration guide</a> and contact the Hudi OSS community for help.</li>
<li>Join the Community: Participate in discussions on the <a href="https://hudi.apache.org/community/get-involved/" target="_blank" rel="noopener noreferrer">Hudi Mailing List</a>, <a href="https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g" target="_blank" rel="noopener noreferrer">Slack</a> and <a href="https://github.com/apache/hudi/issues" target="_blank" rel="noopener noreferrer">GitHub</a>.</li>
<li>Follow us on social media: <a href="https://www.linkedin.com/company/apache-hudi/?viewAsMember=true" target="_blank" rel="noopener noreferrer">Linkedin</a>, <a href="https://twitter.com/ApacheHudi" target="_blank" rel="noopener noreferrer">X/Twitter</a>.</li>
</ul>
<p>We can’t wait to see what you build with Apache Hudi 1.0. Let’s work together to shape the future of data lakehouses!</p>
<p>Crafted with passion for the Apache Hudi community.</p>]]></content:encoded>
            <category>timeline</category>
            <category>design</category>
            <category>release</category>
            <category>streaming ingestion</category>
            <category>multi-writer</category>
            <category>concurrency-control</category>
            <category>blog</category>
        </item>
        <item>
            <title><![CDATA[Introducing Hudi's Non-blocking Concurrency Control for streaming, high-frequency writes]]></title>
            <link>https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control</link>
            <guid>https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control</guid>
            <pubDate>Fri, 06 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>In streaming ingestion scenarios, there are plenty of use cases that require concurrent ingestion from multiple streaming sources.
The user can union all the upstream source inputs into one downstream table to collect the records for unified access across federated queries.
Another very common scenario is multiple stream sources joined together to supplement dimensions of the records to build a wide-dimension table where each source
stream is taking records with partial table schema fields. Common and strong demand for multi-stream concurrent ingestion has always been there.
The Hudi community has collected so many feedbacks from users ever since the day Hudi supported streaming ingestion and processing.</p>
<p>Starting from <a href="https://hudi.apache.org/releases/release-1.0.0" target="_blank" rel="noopener noreferrer">Hudi 1.0.0</a>, we are thrilled to announce a new general-purpose
concurrency model for Apache Hudi - the Non-blocking Concurrency Control (NBCC)- aimed at the stream processing or high-contention/frequent writing scenarios.
In contrast to <a href="https://hudi.apache.org/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic/">Optimistic Concurrency Control</a>, where writers abort the transaction
if there is a hint of contention, this innovation allows multiple streaming writes to the same Hudi table without any overhead of conflict resolution, while
keeping the semantics of <a href="https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/" target="_blank" rel="noopener noreferrer">event-time ordering</a> found in streaming systems, along with
asynchronous table service such as compaction, archiving and cleaning.</p>
<p>NBCC works seamlessly without any new infrastructure or operational overhead. In the subsequent sections of this blog, we will give a brief introduction to Hudi's internals
about the data file layout and TrueTime semantics for time generation, a pre-requisite for discussing NBCC. Following that, we will delve into the design and workflows of NBCC,
and then a simple SQL demo to show the NBCC related config options. The blog will conclude with insights into future work for NBCC.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="older-design">Older Design<a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#older-design" class="hash-link" aria-label="Direct link to Older Design" title="Direct link to Older Design">​</a></h2>
<p>It's important to understand the Hudi <a href="https://hudi.apache.org/docs/next/storage_layouts">storage layout</a> and it evolves/manages data versions. In older release before 1.0.0,
Hudi organizes the data files with units as <code>FileGroup</code>. Each file group contains multiple <code>FileSlice</code>s. Every compaction on this file group generates a new file slice.
Each file slice may comprise an optional base file(columnar file format like Apache Parquet or ORC) and multiple log files(row file format in Apache Avro or Parquet).</p>
<img src="https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/legacy_file_layout.png" alt="Legacy file layout" width="800" align="middle">
<p>The timestamp in the base file name is the instant time of the compaction that writes it, it is also called as "requested instant time" in Hudi's notion.
The timestamp in the log file name is the same timestamp as the current file slice base instant time. Data files with the same instant time belong to one file slice.
In effect, a file group represented a linear ordered sequence of base files (checkpoints) followed by logs files (deltas), followed by base files (checkpoints).</p>
<p>The instant time naming convention in log files becomes a hash limitation in concurrency mode. Each log file contains incremental changes from
multiple commits. Each writer needs to query the file layout to get the base instant time and figure out the full file name before flushing the records.
A more severe problem is the base instant time can be variable with the async compaction pushing forward. In order to make the base instant time deterministic for the log writers, Hudi
forces the schedule sequence between a write commit and compaction scheduling: a compaction can be scheduled only if there is no ongoing ingestion into the Hudi table. Without this, a log file
can be written with a wrong base instant time which could introduce data loss. This means a compaction scheduling could block all the writers in concurrency mode.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="nbcc-design">NBCC Design<a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#nbcc-design" class="hash-link" aria-label="Direct link to NBCC Design" title="Direct link to NBCC Design">​</a></h2>
<p>In order to resolve these pains, since 1.0.0, Hudi introduces a new storage layout based on both requested and completion times of actions, viewing them as an interval.
Each commit in 1.x Hudi has two <a href="https://hudi.apache.org/docs/next/timeline">important notions of time</a>: instant time(or requested time) and completion time.
All the generated timestamp are globally monotonically increasing. Instead of putting the base instant time in the log file name, Hudi now just uses the requested instant time
of the write. During file slicing, Hudi queries the completion time for each log file with the instant time, and we have a new rule for file slicing:</p>
<p><em>A log file belongs to the file slice with the maximum base requested time smaller than(or equals with) it's completion time.</em>[^1]</p>
<img src="https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/new_file_layout.png" alt="New file layout" width="800" align="middle">
<p>With the flexibility of the new file layout, the overhead of querying base instant time is eliminated for log writers and a compaction can be scheduled anywhere with any instant time.
See <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md" target="_blank" rel="noopener noreferrer">RFC-66</a> for more.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="true-time-api">True Time API<a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#true-time-api" class="hash-link" aria-label="Direct link to True Time API" title="Direct link to True Time API">​</a></h3>
<p>In order to ensure the monotonicity of timestamp generation, Hudi introduces the "<a href="https://hudi.apache.org/docs/next/timeline#timeline-components">TrueTime API</a>" since 1.x release.
Basically there are two ways to make the time generation monotonically increasing, inline with TrueTime semantics:</p>
<ul>
<li>A global lock to guard the time generation with mutex, along with a wait for an estimated max allowed clock skew on distributed hosts;</li>
<li>Globally synchronized time generation service, e.g. Google Spanner Time Service, the service itself can ensure the monotonicity.</li>
</ul>
<p>Hudi now implements the "TrueTime" semantics with the first solution, a configurable max waiting time is supported.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="lsm-timeline">LSM timeline<a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#lsm-timeline" class="hash-link" aria-label="Direct link to LSM timeline" title="Direct link to LSM timeline">​</a></h3>
<p>The new file layout requires efficient queries from instant time to get the completion time. Hudi re-implements the archived timeline since 1.x, the
new archived timeline data files are organized as <a href="https://hudi.apache.org/docs/next/timeline#lsm-timeline-history">an LSM tree</a> to support fast time range filtering queries with instant time data-skipping on it.</p>
<img src="https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png" alt="LSM archive timeline" align="middle">
<p>With the powerful new file layout, it is quite straight-forward to implement non-blocking concurrency control. The function is implemented with the simple bucket index on MOR table for Flink.
The bucket index ensures fixed record key to file group mappings for multiple workloads. The log writer writes the records into avro logs and the compaction table service would take care of
the conflict resolution. Because each log file name contains the instant time and each record contains the event time ordering field, Hudi reader can merge the records either
with natural order(processing time sequence) or event time order.</p>
<p>The concurrency mode should be configured as <code>NON_BLOCKING_CONCURRENCY_CONTROL</code>, you can enable the table services on one job and disable it for the others.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="flink-sql-demo">Flink SQL demo<a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#flink-sql-demo" class="hash-link" aria-label="Direct link to Flink SQL demo" title="Direct link to Flink SQL demo">​</a></h2>
<p>Here is a demo to show 2 pipelines that ingest into the same downstream table, the two sink table views share the same table path.</p>
<div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)">-- NB-CC demo</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- The source table</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">TABLE</span><span class="token plain"> sourceT </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  uuid </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  name </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  age </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">partition</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">as</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'par1'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">WITH</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'datagen'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'rows-per-second'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'200'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- table view for writer1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">create</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">table</span><span class="token plain"> t1</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  uuid </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  name </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  age </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">partition</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'hudi'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'path'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'/Users/chenyuzhao/workspace/hudi-demo/t1'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'table.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'MERGE_ON_READ'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'index.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'BUCKET'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'hoodie.write.concurrency.mode'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'NON_BLOCKING_CONCURRENCY_CONTROL'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'write.tasks'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'2'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token plain"> t1</span><span class="token comment" style="color:rgb(98, 114, 164)">/*+options('metadata.enabled'='true')*/</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> sourceT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- table view for writer2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- compaction and cleaning are disabled because writer1 has taken care of it.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">create</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">table</span><span class="token plain"> t1_2</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  uuid </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  name </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">10</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  age </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">int</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ts </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">3</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token identifier">partition</span><span class="token identifier punctuation" style="color:rgb(248, 248, 242)">`</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">varchar</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token number">20</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">with</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'connector'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'hudi'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'path'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'/Users/chenyuzhao/workspace/hudi-demo/t1'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'table.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'MERGE_ON_READ'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'index.type'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'BUCKET'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'hoodie.write.concurrency.mode'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'NON_BLOCKING_CONCURRENCY_CONTROL'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'write.tasks'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'2'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'compaction.schedule.enabled'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'false'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'compaction.async.enabled'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'false'</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token string" style="color:rgb(255, 121, 198)">'clean.async.enabled'</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'false'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)">-- executes the ingestion workloads</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token plain"> t1 </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> sourceT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">insert</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">into</span><span class="token plain"> t1_2 </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">select</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">from</span><span class="token plain"> sourceT</span><span class="token punctuation" style="color:rgb(248, 248, 242)">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-roadmap">Future Roadmap<a href="https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#future-roadmap" class="hash-link" aria-label="Direct link to Future Roadmap" title="Direct link to Future Roadmap">​</a></h2>
<p>While non-blocking concurrency control is a very powerful feature for streaming users, it is a general solution for multiple writer conflict resolution,
here are some plans that improve the Hudi core features:</p>
<ul>
<li>NBCC support for metadata table</li>
<li>NBCC for clustering</li>
<li>NBCC for other index type</li>
</ul>
<hr>
<p>[^1] <a href="https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md" target="_blank" rel="noopener noreferrer">RFC-66</a> well-explained the completion time based file slicing with a pseudocode.</p>]]></content:encoded>
            <category>design</category>
            <category>streaming ingestion</category>
            <category>multi-writer</category>
            <category>concurrency-control</category>
            <category>blog</category>
        </item>
        <item>
            <title><![CDATA[Hudi’s Automatic File Sizing Delivers Unmatched Performance]]></title>
            <link>https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling</link>
            <guid>https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling</guid>
            <pubDate>Tue, 19 Nov 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Introduction]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction">​</a></h2>
<p>In today’s data-driven world, managing large volumes of data efficiently is crucial. One of the standout features of Apache Hudi is its ability to handle small files during data writes, which significantly optimizes both performance and cost. In this post, we’ll explore how Hudi’s auto file sizing, powered by a unique bin packing algorithm, can transform your data processing workflows.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="understanding-small-file-challenges">Understanding Small File Challenges<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#understanding-small-file-challenges" class="hash-link" aria-label="Direct link to Understanding Small File Challenges" title="Direct link to Understanding Small File Challenges">​</a></h2>
<p>In big data environments, small files can pose a major challenge. Some major use-cases which can create lot of small files -</p>
<ul>
<li><strong>Streaming Workloads</strong> :
When data is ingested in micro-batches, as is common in streaming workloads, the resulting files tend to be small. This can lead to a significant number of small files, especially for high-throughput streaming applications.</li>
<li><strong>High-Cardinality Partitioning</strong> :
Excessive partitioning, particularly on columns with high cardinality, can create a large number of small files. This can be especially problematic when dealing with large datasets and complex data schemas.</li>
</ul>
<p>These small files can lead to several inefficiencies that can include increased metadata overhead, degraded read performance, and higher storage costs, particularly when using cloud storage solutions like Amazon S3.</p>
<ul>
<li><strong>Increased Metadata Overhead</strong> :
Metadata is data about data, including information such as file names, sizes, creation dates, and other attributes that help systems manage and locate files. Each file, no matter how small, requires metadata to be tracked and managed. In environments where numerous small files are created, the amount of metadata generated can skyrocket. For instance, if a dataset consists of thousands of tiny files, the system must maintain metadata for each of these files. This can overwhelm metadata management systems, leading to longer lookup times and increased latency when accessing files.</li>
<li><strong>Degraded Read Performance</strong> :
Reading data from storage typically involves input/output (I/O) operations, which can be costly in terms of time and resources. When files are small, the number of I/O operations increases, as each small file needs to be accessed individually. This scenario can create bottlenecks, particularly in analytical workloads where speed is critical. Querying a large number of small files may result in significant delays, as the system spends more time opening and reading each file than processing the data itself.</li>
<li><strong>Higher Cloud Costs</strong> :
Many cloud storage solutions, like Amazon S3, charge based on the total amount of data stored as well as the number of requests made. With numerous small files, not only does the total storage requirement increase, but the number of requests to access these files also grows. Each small file incurs additional costs due to the overhead associated with managing and accessing them. This can add up quickly, leading to unexpectedly high storage bills.</li>
<li><strong>High Query Load</strong> :
Multiple teams are querying these tables for various dashboards, ad-hoc analyses, and machine learning tasks. This leads to a high number of concurrent queries, including Spark jobs, which can significantly impact performance. All those queries/jobs will take a hit on both performance and cost.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="impact-of-small-file">Impact of Small File<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#impact-of-small-file" class="hash-link" aria-label="Direct link to Impact of Small File" title="Direct link to Impact of Small File">​</a></h3>
<p>To demonstrate the impact of small files, we conducted a benchmarking using AWS EMR.
Dataset Used - TPC-DS 1 TB dataset ( <a href="https://www.tpc.org/tpcds/" target="_blank" rel="noopener noreferrer">https://www.tpc.org/tpcds/</a> )
Cluster Configurations - 10 nodes (m5.4xlarge)
Spark Configurations - Executors: 10 (16 cores 32 GB memory)
Dataset Generation - We generated two types of datasets in parquet format</p>
<ul>
<li>Optimized File Sizes which had ~100 MB sized files</li>
<li>Small File Sizes which had ~5-10 MB sized files
Execution and Results</li>
<li>We executed 3 rounds of 99 standard TPC-DS queries on both datasets and measured the time taken by the queries.</li>
<li>The results indicated that queries executed on small files were, on average, 30% slower compared to those executed on optimized file sizes.</li>
</ul>
<p>The following chart illustrates the average runtimes for the 99 queries across each round.</p>
<p><img decoding="async" loading="lazy" alt="Impact of Small Files" src="https://hudi.apache.org/assets/images/2024-11-19-automated-small-file-handling-benchmarks-5340e7e5e0e586c3803f6e06796b5daf.png" width="3188" height="1844" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-table-formats-solve-this-problem">How table formats solve this problem<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#how-table-formats-solve-this-problem" class="hash-link" aria-label="Direct link to How table formats solve this problem" title="Direct link to How table formats solve this problem">​</a></h2>
<p>When it comes to managing small files in table formats, there are two primary strategies:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ingesting-data-as-is-and-optimizing-post-ingestion-"><strong>Ingesting Data As-Is and Optimizing Post-Ingestion</strong> :<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#ingesting-data-as-is-and-optimizing-post-ingestion-" class="hash-link" aria-label="Direct link to ingesting-data-as-is-and-optimizing-post-ingestion-" title="Direct link to ingesting-data-as-is-and-optimizing-post-ingestion-">​</a></h3>
<p>In this approach, data, including small files, is initially ingested without immediate processing. After ingestion, various technologies provide functionalities to merge these small files into larger, more efficient partitions:</p>
<ul>
<li>Hudi uses clustering to manage small files.</li>
<li>Delta Lake utilizes the OPTIMIZE command.</li>
<li>Iceberg offers the rewrite_data_files function.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pros">Pros:<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#pros" class="hash-link" aria-label="Direct link to Pros:" title="Direct link to Pros:">​</a></h4>
<ul>
<li>Writing small files directly accelerates the ingestion process, enabling quick data availability—especially beneficial for real-time or near-real-time applications.</li>
<li>The initial write phase involves less data manipulation, as small files are simply appended. This streamlines workflows and eases the management of incoming data streams.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="cons">Cons:<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#cons" class="hash-link" aria-label="Direct link to Cons:" title="Direct link to Cons:">​</a></h4>
<ul>
<li>Until clustering or optimization is performed, small files may be exposed to readers, which can significantly slow down queries and potentially violate read SLAs.</li>
<li>Just like with read performance, exposing small files to readers can lead to a high number of cloud storage API calls, which can increase cloud costs significantly.</li>
<li>Managing table service jobs can become cumbersome. These jobs often can't run in parallel with ingestion tasks, leading to potential delays and resource contention.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="managing-small-files-during-ingestion-only-"><strong>Managing Small Files During Ingestion Only</strong> :<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#managing-small-files-during-ingestion-only-" class="hash-link" aria-label="Direct link to managing-small-files-during-ingestion-only-" title="Direct link to managing-small-files-during-ingestion-only-">​</a></h3>
<p>Hudi offers a unique functionality that can handle small files during the ingestion only, ensuring that only larger files are stored in the table. This not only optimizes read performance but also significantly reduces storage costs.
By eliminating small files from the lake, Hudi addresses key challenges associated with data management, providing a streamlined solution that enhances both performance and cost efficiency.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-hudi-helps-in-small-file-handling-during-ingestion">How Hudi helps in small file handling during ingestion<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#how-hudi-helps-in-small-file-handling-during-ingestion" class="hash-link" aria-label="Direct link to How Hudi helps in small file handling during ingestion" title="Direct link to How Hudi helps in small file handling during ingestion">​</a></h2>
<p>Hudi automatically manages file sizing during insert and upsert operations. It employs a bin packing algorithm to handle small files effectively. A bin packing algorithm is a technique used to optimize file storage by grouping files of varying sizes into fixed-size containers, often referred to as "bins." This strategy aims to minimize the number of bins required to store all files efficiently. When writing data, Hudi identifies file groups of small files and merges new data into the same  group, resulting in optimized file sizes.</p>
<p>The diagram above illustrates how Hudi employs a bin packing algorithm to manage small files while using default parameters: a small file limit of 100 MB and a maximum file size of 120 MB.</p>
<p><img decoding="async" loading="lazy" alt="  " src="https://hudi.apache.org/assets/images/2024-11-19-automated-small-file-handling-process-676b9be484af36088162dfaf6a219a1f.png" width="1350" height="632" class="img_ev3q"></p>
<p>Initially, the table contains the following files: F1 (110 MB), F2 (60 MB), F3 (20 MB), and F4 (20 MB).
After processing a batch-1 of 150 MB, F2, F3, and F4 will all be classified as small files since they each fall below the 100 MB threshold. The first 60 MB will be allocated to F2, increasing its size to 120 MB. The remaining 90 MB will be assigned to F3, bringing its total to 110 MB.
After processing batch-2 of 150 MB, only F4 will be classified as a small file. F3, now at 110 MB, will not be considered a small file since it exceeds the 100 MB limit. Therefore, an additional 100 MB will be allocated to F4, increasing its size to 120 MB, while the remaining 50 MB will create a new file of 50 MB.
We can refer this blog for in-depth details of the functionality  - <a href="https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/</a></p>
<p>We use following configs to configure this -</p>
<ul>
<li>
<p><strong>hoodie.parquet.max.file.size (Default 128 MB)</strong>
This setting specifies the target size, in bytes, for Parquet files generated during Hudi write phases. The writer will attempt to create files that approach this target size. For example, if an existing file is 80 MB, the writer will allocate only 40 MB to that particular file group.</p>
</li>
<li>
<p><strong>hoodie.parquet.small.file.limit (Default 100 MB)</strong>
This setting defines the maximum file size for a data file to be classified as a small file. Files below this threshold are considered small files, prompting the system to allocate additional records to their respective file groups in subsequent write phases.</p>
</li>
<li>
<p><strong>hoodie.copyonwrite.record.size.estimate (Default 1024)</strong>
This setting represents the estimated average size of a record. If not explicitly specified, Hudi will dynamically compute this estimate based on commit metadata. Accurate record size estimation is essential for determining insert parallelism and efficiently bin-packing inserts into smaller files.</p>
</li>
<li>
<p><strong>hoodie.copyonwrite.insert.split.size (Default 500000)</strong>
This setting determines the number of records inserted into each partition or bucket during a write operation. The default value is based on the assumption of 100MB files with at least 1KB records, resulting in approximately 100,000 records per file. To accommodate potential variations, we overprovision to 500,000 records. As long as auto-tuning of splits is turned on, this only affects the first write, where there is no history to learn record sizes from.</p>
</li>
<li>
<p><strong>hoodie.merge.small.file.group.candidates.limit (Default1)</strong>
This setting specifies the maximum number of file groups whose base files meet the small-file limit that can be considered for appending records during an upsert operation. This parameter is applicable only to Merge-On-Read (MOR) tables.</p>
</li>
</ul>
<p>We can refer this blog to understand internal functionality how it works -
<a href="https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>Hudi's innovative approach to managing small files during ingestion positions it as a compelling choice in the lakehouse landscape. By automatically merging small files at the time of ingestion, it optimizes storage costs and enhances read performance, and alleviates users from the operational burden of maintaining their tables in an optimized state.</p>
<p>Unleash the power of Apache Hudi for your big data challenges! Head over to <a href="https://hudi.apache.org/" target="_blank" rel="noopener noreferrer">https://hudi.apache.org/</a> and dive into the quickstarts to get started. Want to learn more? Join our vibrant Hudi community! Attend the monthly Community Call or hop into the Apache Hudi Slack to ask questions and gain deeper insights.</p>]]></content:encoded>
            <category>Data Lake</category>
            <category>Apache Hudi</category>
        </item>
        <item>
            <title><![CDATA[Record Level Indexing in Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2024/11/12/record-level-indexing-in-apache-hudi</link>
            <guid>https://hudi.apache.org/blog/2024/11/12/record-level-indexing-in-apache-hudi</guid>
            <pubDate>Tue, 12 Nov 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/@prasadpal107/record-level-indexing-in-apache-hudi-0615804608ec">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>record index</category>
            <category>record level index</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[Storing 200 Billion Entities: Notion’s Data Lake Project]]></title>
            <link>https://hudi.apache.org/blog/2024/11/12/storing-200-billion-entities-notions</link>
            <guid>https://hudi.apache.org/blog/2024/11/12/storing-200-billion-entities-notions</guid>
            <pubDate>Tue, 12 Nov 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.bytebytego.com/p/storing-200-billion-entities-notions">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>use-case</category>
            <category>bytebytego</category>
        </item>
        <item>
            <title><![CDATA[Understanding COW and MOR in Apache Hudi: Choosing the Right Storage Strategy]]></title>
            <link>https://hudi.apache.org/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi</link>
            <guid>https://hudi.apache.org/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi</guid>
            <pubDate>Tue, 12 Nov 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://opstree.com/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi-choosing-the-right-storage-strategy/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>cow</category>
            <category>mor</category>
            <category>opstree</category>
        </item>
        <item>
            <title><![CDATA[I spent 5 hours exploring the story behind Apache Hudi.]]></title>
            <link>https://hudi.apache.org/blog/2024/10/27/I-spent-5-hours-exploring-the-story-behind-Apache-Hudi</link>
            <guid>https://hudi.apache.org/blog/2024/10/27/I-spent-5-hours-exploring-the-story-behind-Apache-Hudi</guid>
            <pubDate>Sun, 27 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.det.life/i-spent-5-hours-exploring-the-story-behind-apache-hudi-dacad829394d">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>beginner</category>
            <category>det</category>
        </item>
        <item>
            <title><![CDATA[Moving Large Tables from Snowflake to S3 Using the COPY INTO Command and Hudi Bootstrapping to Build Data Lakes | Hands-On Labs]]></title>
            <link>https://hudi.apache.org/blog/2024/10/26/moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi</link>
            <guid>https://hudi.apache.org/blog/2024/10/26/moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi</guid>
            <pubDate>Sat, 26 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/moving-large-tables-from-snowflake-s3-using-copy-command-soumil-shah-csdse/?trackingId=8qFtCUc3R7CAo%2BP883rgUA%3D%3D">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>aws s3</category>
            <category>bootstrap</category>
            <category>linkedin</category>
        </item>
        <item>
            <title><![CDATA[Using Apache Hudi with Apache Flink]]></title>
            <link>https://hudi.apache.org/blog/2024/10/23/Using-Apache-Hudi-with-Apache-Flink</link>
            <guid>https://hudi.apache.org/blog/2024/10/23/Using-Apache-Hudi-with-Apache-Flink</guid>
            <pubDate>Wed, 23 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/tutorial-hudi-for-flink.html">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>apache flink</category>
            <category>beginner</category>
            <category>aws</category>
            <category>amazon</category>
        </item>
        <item>
            <title><![CDATA[Mastering Open Table Formats: A Guide to Apache Iceberg, Hudi, and Delta Lake]]></title>
            <link>https://hudi.apache.org/blog/2024/10/23/mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake</link>
            <guid>https://hudi.apache.org/blog/2024/10/23/mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake</guid>
            <pubDate>Wed, 23 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://medium.com/itversity/understanding-open-table-formats-a-comprehensive-guide-ba6f072167fb">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Iceberg</category>
            <category>Delta Lake</category>
            <category>comparison</category>
            <category>medium</category>
        </item>
        <item>
            <title><![CDATA[Exploring Time Travel Queries in Apache Hudi]]></title>
            <link>https://hudi.apache.org/blog/2024/10/22/exploring-time-travel-queries-in-apache-hudi</link>
            <guid>https://hudi.apache.org/blog/2024/10/22/exploring-time-travel-queries-in-apache-hudi</guid>
            <pubDate>Tue, 22 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://opstree.com/blog/2024/10/22/time-travel-queries-in-apache-hudi/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>time travel query</category>
            <category>opstree</category>
        </item>
        <item>
            <title><![CDATA[Streaming DynamoDB Data into a Hudi Table: AWS Glue in Action]]></title>
            <link>https://hudi.apache.org/blog/2024/10/14/streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action</link>
            <guid>https://hudi.apache.org/blog/2024/10/14/streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action</guid>
            <pubDate>Mon, 14 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.antstack.com/blog/Streaming-DynamoDB-Data-into-a-Hudi-Table/">here</a></span>]]></content:encoded>
            <category>how-to</category>
            <category>Apache Hudi</category>
            <category>amazon s3</category>
            <category>aws glue</category>
            <category>amazon kinesis</category>
            <category>amazon dynamodb</category>
            <category>antstack</category>
        </item>
        <item>
            <title><![CDATA[Iceberg vs. Delta Lake vs. Hudi: A Comparative Look at Lakehouse Architectures]]></title>
            <link>https://hudi.apache.org/blog/2024/10/07/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures</link>
            <guid>https://hudi.apache.org/blog/2024/10/07/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures</guid>
            <pubDate>Mon, 07 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.forefathers.io/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures-52eec62b29e8">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Iceberg</category>
            <category>Delta Lake</category>
            <category>comparison</category>
            <category>forefathers</category>
        </item>
        <item>
            <title><![CDATA[Mastering Slowly Changing Dimensions with Apache Hudi & Spark SQL]]></title>
            <link>https://hudi.apache.org/blog/2024/10/07/mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql</link>
            <guid>https://hudi.apache.org/blog/2024/10/07/mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql</guid>
            <pubDate>Mon, 07 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://www.linkedin.com/pulse/mastering-slowly-changing-dimensions-apache-hudi-spark-sameer-shaik-7zkjf/?trackingId=1qCeO8FIRJy32LcpHIvy3Q%3D%3D">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>scd1</category>
            <category>scd2</category>
            <category>scd3</category>
            <category>spark-sql</category>
            <category>linkedin</category>
        </item>
        <item>
            <title><![CDATA[Apache Hudi, Spark and Minio: Hands-on Lab in Docker]]></title>
            <link>https://hudi.apache.org/blog/2024/10/02/apache-hudi-spark-and-minio-hands-on-lab-in-docker</link>
            <guid>https://hudi.apache.org/blog/2024/10/02/apache-hudi-spark-and-minio-hands-on-lab-in-docker</guid>
            <pubDate>Wed, 02 Oct 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.devgenius.io/apache-hudi-spark-and-minio-hands-on-lab-in-docker-f1daa099ccd0">here</a></span>]]></content:encoded>
            <category>how-to</category>
            <category>Apache Hudi</category>
            <category>Apache Spark</category>
            <category>Minio</category>
            <category>docker</category>
            <category>devgenius</category>
        </item>
        <item>
            <title><![CDATA[Change query support in Apache Hudi (0.15)]]></title>
            <link>https://hudi.apache.org/blog/2024/09/30/change-query-support-in-apache-hudi-0-15</link>
            <guid>https://hudi.apache.org/blog/2024/09/30/change-query-support-in-apache-hudi-0-15</guid>
            <pubDate>Mon, 30 Sep 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://jack-vanlightly.com/analyses/2024/9/27/change-query-support-in-apache-hudi">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>CDC</category>
            <category>Change Data Capture</category>
            <category>jack-vanlightly</category>
        </item>
        <item>
            <title><![CDATA[Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared]]></title>
            <link>https://hudi.apache.org/blog/2024/09/24/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared</link>
            <guid>https://hudi.apache.org/blog/2024/09/24/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared</guid>
            <pubDate>Tue, 24 Sep 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://lakefs.io/blog/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>apache hudi</category>
            <category>apache iceberg</category>
            <category>delta lake</category>
            <category>comparison</category>
            <category>lakefs</category>
        </item>
        <item>
            <title><![CDATA[Hands-on with Apache Hudi and Spark]]></title>
            <link>https://hudi.apache.org/blog/2024/09/22/hands-on-with-apache-hudi-and-spark</link>
            <guid>https://hudi.apache.org/blog/2024/09/22/hands-on-with-apache-hudi-and-spark</guid>
            <pubDate>Sun, 22 Sep 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Redirecting... please wait!!]]></description>
            <content:encoded><![CDATA[<span>Redirecting... please wait!! <!-- -->or click <a href="https://blog.devgenius.io/hands-on-with-apache-hudi-ce45869b5eff">here</a></span>]]></content:encoded>
            <category>blog</category>
            <category>Apache Hudi</category>
            <category>Apache Spark</category>
            <category>devgenius</category>
        </item>
    </channel>
</rss>