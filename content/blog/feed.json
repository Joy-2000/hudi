{
    "version": "https://jsonfeed.org/version/1",
    "title": "Apache Hudi: User-Facing Analytics",
    "home_page_url": "https://hudi.apache.org/blog",
    "description": "Apache Hudi Blog",
    "items": [
        {
            "id": "https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"overview\">Overview<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#overview\" class=\"hash-link\" aria-label=\"Direct link to Overview\" title=\"Direct link to Overview\">​</a></h2>\n<p>We are thrilled to announce the release of Apache Hudi 1.0, a landmark achievement for our vibrant community that defines what the next generation of data lakehouses should achieve. Hudi pioneered <em><strong>transactional data lakes</strong></em> in 2017, and today, we live in a world where this technology category is mainstream as the “<em><strong>Data Lakehouse”</strong></em>. The Hudi community has made several key, original, and first-of-its-kind contributions to this category, as shown below, compared to when other OSS alternatives emerged. This is an incredibly rare feat for a relatively small OSS community to sustain in a fiercely competitive commercial data ecosystem. On the other hand, it also demonstrates the value of deeply understanding the technology category within a focused open-source community. So, I first want to thank/congratulate the Hudi community and the <strong>60+ contributors</strong> for making 1.0 happen.</p>\n<div style=\"text-align:center\"><img src=\"https://hudi.apache.org/assets/images/blog/hudi-innovation-timeline.jpg\" alt=\"innovation timeline\"></div>\n<p>This <a href=\"https://hudi.apache.org/releases/release-1.0.0\">release</a> is more than just a version increment—it advances the breadth of Hudi’s feature set and its architecture's robustness while bringing fresh innovation to shape the future. This post reflects on how technology and the surrounding ecosystem have evolved, making a case for a holistic “<em><strong>Data Lakehouse Management System</strong></em>” (<em><strong>DLMS</strong></em>) as the new Northstar. For most of this post, we will deep dive into the latest capabilities of Hudi 1.0 that make this evolution possible.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"evolution-of-the-data-lakehouse\">Evolution of the Data Lakehouse<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#evolution-of-the-data-lakehouse\" class=\"hash-link\" aria-label=\"Direct link to Evolution of the Data Lakehouse\" title=\"Direct link to Evolution of the Data Lakehouse\">​</a></h2>\n<p>Technologies must constantly evolve—<a href=\"https://en.wikipedia.org/wiki/Web3\" target=\"_blank\" rel=\"noopener noreferrer\">Web 3.0</a>, <a href=\"https://en.wikipedia.org/wiki/List_of_wireless_network_technologies\" target=\"_blank\" rel=\"noopener noreferrer\">cellular tech</a>, <a href=\"https://en.wikipedia.org/wiki/Programming_language_generations\" target=\"_blank\" rel=\"noopener noreferrer\">programming language generations</a>—based on emerging needs. Data lakehouses are no exception. This section explores the hierarchy of such needs for data lakehouse users. The most basic need is the “<strong>table format</strong>” functionality, the foundation for data lakehouses. Table format organizes the collection of files/objects into tables with snapshots, schema, and statistics tracking, enabling higher abstraction. Furthermore, table format dictates the organization of files within each snapshot, encoding deletes/updates and metadata about how the table changes over time. Table format also provides protocols for various readers and writers and table management processes to handle concurrent access and provide ACID transactions safely. In the last five years, leading data warehouse and cloud vendors have integrated their proprietary SQL warehouse stack with open table formats. While they mostly default to their closed table formats and the compute engines remain closed, this welcome move provides users an open alternative for their data.</p>\n<p>However, the benefits of a format end there, and now a table format is just the tip of the iceberg. Users require an <a href=\"https://www.onehouse.ai/blog/open-table-formats-and-the-open-data-lakehouse-in-perspective\" target=\"_blank\" rel=\"noopener noreferrer\">end-to-end open data lakehouse</a>, and modern data lakehouse features need a sophisticated layer of <em><strong>open-source software</strong></em> operating on data stored in open table formats. For example, Optimized writers can balance cost and performance by carefully managing file sizes using the statistics maintained in the table format or catalog syncing service that can make data in Hudi readily available to half a dozen catalogs open and closed out there. Hudi shines by providing a high-performance open table format as well as a comprehensive open-source software stack that can ingest, store, optimize and effectively self-manage a data lakehouse. This distinction between open formats and open software is often lost in translation inside the large vendor ecosystem in which Hudi operates. Still, it has been and remains a key consideration for Hudi’s <a href=\"https://hudi.apache.org/powered-by\">users</a> to avoid compute-lockin to any given data vendor. The Hudi streamer tool, e.g., powers hundreds of data lakes by ingesting data seamlessly from various sources at the convenience of a single command in a terminal.</p>\n<div style=\"text-align:center;width:90%;height:auto\"><img src=\"https://hudi.apache.org/assets/images/blog/dlms-hierarchy.png\" alt=\"dlms hierarchy\"></div>\n<p>Moving forward with 1.0, the community has <a href=\"https://github.com/apache/hudi/pull/8679\" target=\"_blank\" rel=\"noopener noreferrer\">debated</a> these key points and concluded that we need more open-source “<strong>software capabilities</strong>” that are directly comparable with DBMSes for two main reasons.</p>\n<p><strong>Significantly expand the technical capabilities of a data lakehouse</strong>: Many design decisions in Hudi have been inspired by databases (see <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-69/rfc-69.md#hudi-1x\" target=\"_blank\" rel=\"noopener noreferrer\">here</a> for a layer-by-layer mapping) and have delivered significant benefits to the community. For example, Hudi’s indexing mechanisms deliver the fast update performance the project has come to be known for.  We want to generalize such features across writers and queries and introduce new capabilities like fast metastores for query planning, support for unstructured/multimodal data and caching mechanisms that can be deeply integrated into (at least) open-source query engines in the ecosystem. We also need concurrency control that works for lakehouse workloads instead of employing techniques applicable to OLTP databases at the surface level.</p>\n<p><strong>We also need a database-like experience</strong>: We originally designed Hudi as a software library that can be embedded into different query/processing engines for reading/writing/managing tables. This model has been a great success within the existing data ecosystem, which is familiar with scheduling jobs and employing multiple engines for ETL and interactive queries. However, for a new user wanting to explore data lakehouses, there is no piece of software to easily install and explore all functionality packaged coherently. Such data lakehouse functionality packaged and delivered like a typical database system unlocks new use cases. For example, with such a system, we could bring HTAP capabilities to the data lakehouses on faster cloud storage/row-oriented formats, finally making it a low-latency data serving layer.</p>\n<p>If combined, we would gain a powerful database built on top of the data lake(house) architecture—a <em><strong>data</strong></em> <em><strong>lakehouse</strong></em> <em><strong>management</strong></em> <em><strong>system</strong></em> <em><strong>(DLMS)</strong></em>—that we believe the industry needs.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"key-features-in-hudi-10\">Key Features in Hudi 1.0<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#key-features-in-hudi-10\" class=\"hash-link\" aria-label=\"Direct link to Key Features in Hudi 1.0\" title=\"Direct link to Key Features in Hudi 1.0\">​</a></h2>\n<p>In Hudi 1.0, we’ve delivered a significant expansion of data lakehouse technical capabilities discussed above inside Hudi’s <a href=\"https://en.wikipedia.org/wiki/Database_engine\" target=\"_blank\" rel=\"noopener noreferrer\">storage engine</a> layer.  Storage engines (a.k.a database engines) are standard database components that sit on top of the storage/file/table format and are wrapped by the DBMS layer above, handling the core read/write/management functionality. In the figure below, we map the Hudi components with the seminal <a href=\"https://dsf.berkeley.edu/papers/fntdb07-architecture.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Architecture of a Database System</a> paper (see page 4) to illustrate the standard layering discussed. If the layering is implemented correctly, we can deliver the benefits of the storage engine to even other table formats, which may lack such fully-developed open-source software for table management or achieving high performance, via interop standards defined in projects like <a href=\"https://xtable.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Apache XTable (Incubating)</a>.</p>\n<div style=\"text-align:center;width:80%;height:auto\"><img src=\"https://hudi.apache.org/assets/images/hudi-stack-1-x.png\" alt=\"Hudi DB Architecture\"><p align=\"center\">Figure: Apache Hudi Database Architecture</p></div>\n<p>Regarding full-fledged DLMS functionality, the closest experience Hudi 1.0 offers is through Apache Spark. Users can deploy a Spark server (or Spark Connect) with Hudi 1.0 installed, submit SQL/jobs, orchestrate table services via SQL commands, and enjoy new secondary index functionality to speed up queries like a DBMS. Subsequent releases in the 1.x release line and beyond will continuously add new features and improve this experience.</p>\n<p>In the following sections, let’s dive into what makes Hudi 1.0 a standout release.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"new-time-and-timeline\">New Time and Timeline<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#new-time-and-timeline\" class=\"hash-link\" aria-label=\"Direct link to New Time and Timeline\" title=\"Direct link to New Time and Timeline\">​</a></h3>\n<p>For the familiar user, time is a key concept in Hudi. Hudi’s original notion of time was instantaneous, i.e., actions that modify the table appear to take effect at a given instant. This was limiting when designing features like non-blocking concurrency control across writers, which needs to reason about actions more as an “interval” to detect other conflicting actions. Every action on the Hudi timeline now gets a <em>requested</em> and a <em>completion</em> time; Thus, the timeline layout version has bumped up in the 1.0 release. Furthermore, to ease the understanding and bring consistency around time generation for users and implementors, we have formalized the adoption of <a href=\"https://hudi.apache.org/docs/timeline#truetime-generation\">TrueTime</a> semantics. The default implementation assures forward-moving clocks even with distributed processes, assuming a maximum tolerable clock skew similar to <a href=\"https://cockroachlabs.com/blog/living-without-atomic-clocks/\" target=\"_blank\" rel=\"noopener noreferrer\">OLTP/NoSQL</a> stores adopting TrueTime.</p>\n<div style=\"text-align:center\"><img src=\"https://hudi.apache.org/assets/images/hudi-timeline-actions.png\" alt=\"Timeline actions\"><p align=\"center\">Figure: Showing actions in Hudi 1.0 modeled as an interval of two instants: requested and completed</p></div>\n<p>Hudi tables are frequently updated, and users also want to retain a more extended action history associated with the table. Before Hudi 1.0, the older action history in a table was archived for audit access. But, due to the lack of support for cloud storage appends, access might become cumbersome due to tons of small files. In Hudi 1.0, we have redesigned the timeline as an <a href=\"https://en.wikipedia.org/wiki/Log-structured_merge-tree\" target=\"_blank\" rel=\"noopener noreferrer\">LSM tree</a>, which is widely adopted for cases where good write performance on temporal data is desired.</p>\n<p>In the Hudi 1.0 release, the <a href=\"https://hudi.apache.org/docs/timeline#lsm-timeline-history\">LSM timeline</a> is heavily used in the query planning to map requested and completion times across Apache Spark, Apache Flink and Apache Hive. Future releases plan to leverage this to unify the timeline's active and history components, providing infinite retention of table history. Micro benchmarks show that the LSM timeline can be pretty efficient, even committing every <em><strong>30 seconds for 10 years with about 10M instants</strong></em>, further cementing Hudi’s table format as the most suited for frequently written tables.</p>\n<table><thead><tr><th style=\"text-align:left\">Number of actions</th><th style=\"text-align:left\">Instant Batch Size</th><th style=\"text-align:left\">Read cost (just times)</th><th style=\"text-align:left\">Read cost \u000b(along with action metadata)</th><th style=\"text-align:left\">Total file size</th></tr></thead><tbody><tr><td style=\"text-align:left\">10000</td><td style=\"text-align:left\">10</td><td style=\"text-align:left\">32ms</td><td style=\"text-align:left\">150ms</td><td style=\"text-align:left\">8.39MB</td></tr><tr><td style=\"text-align:left\">20000</td><td style=\"text-align:left\">10</td><td style=\"text-align:left\">51ms</td><td style=\"text-align:left\">188ms</td><td style=\"text-align:left\">16.8MB</td></tr><tr><td style=\"text-align:left\">10000000</td><td style=\"text-align:left\">1000</td><td style=\"text-align:left\">3400ms</td><td style=\"text-align:left\">162s</td><td style=\"text-align:left\">8.4GB</td></tr></tbody></table>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"secondary-indexing-for-faster-lookups\">Secondary Indexing for Faster Lookups<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#secondary-indexing-for-faster-lookups\" class=\"hash-link\" aria-label=\"Direct link to Secondary Indexing for Faster Lookups\" title=\"Direct link to Secondary Indexing for Faster Lookups\">​</a></h3>\n<p>Indexes are core to Hudi’s design, so much so that even the first pre-open-source version of Hudi shipped with <a href=\"https://hudi.apache.org/docs/indexes#additional-writer-side-indexes\">indexes</a> to speed up writes. However, these indexes were limited to the writer's side, except for record indexes in 0.14+ above, which were also integrated with Spark SQL queries. Hudi 1.0 generalizes indexes closer to the indexing functionality found in relational databases, supporting indexes on any secondary column across both writer and readers. Hudi 1.0 also supports near-standard <a href=\"https://hudi.apache.org/docs/sql_ddl#create-index\">SQL syntax</a> for creating/dropping indexes on different columns via Spark SQL, along with an asynchronous indexing table service to build indexes without interrupting the writers.</p>\n<div style=\"text-align:center;padding-left:10%;width:70%;height:auto\"><img src=\"https://hudi.apache.org/assets/images/hudi-stack-indexes.png\" alt=\"Indexes\"><p align=\"center\">Figure: the indexing subsystem in Hudi 1.0, showing different types of indexes</p></div>\n<p>With secondary indexes, queries and DMLs scan a much-reduced amount of files from cloud storage, dramatically reducing costs (e.g., on engines like AWS Athena, which price by data scanned) and improving query performance for queries with low to even moderate amount of selectivity. On a benchmark of a query on <em>web_sales</em> table (from <em><strong>10 TB tpc-ds dataset</strong></em>), with file groups - 286,603, total records - 7,198,162,544 and cardinality of secondary index column in the ~ 1:150 ranges, we see a remarkable <em><strong>~95% decrease in latency</strong></em>.</p>\n<table><thead><tr><th style=\"text-align:left\">Run 1</th><th style=\"text-align:left\">Total Query Latency w/o indexing skipping (secs)</th><th style=\"text-align:left\">Total Query Latency with secondary index skipping (secs)</th><th style=\"text-align:left\">% decrease</th></tr></thead><tbody><tr><td style=\"text-align:left\">1</td><td style=\"text-align:left\">252</td><td style=\"text-align:left\">31</td><td style=\"text-align:left\">~88%</td></tr><tr><td style=\"text-align:left\">2</td><td style=\"text-align:left\">214</td><td style=\"text-align:left\">10</td><td style=\"text-align:left\">~95%</td></tr><tr><td style=\"text-align:left\">3</td><td style=\"text-align:left\">204</td><td style=\"text-align:left\">9</td><td style=\"text-align:left\">~95%</td></tr></tbody></table>\n<p>In Hudi 1.0, secondary indexes are only supported for Apache Spark, with planned support for other engines in Hudi 1.1, starting with Flink, Presto and Trino.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"bloom-filter-indexes\">Bloom Filter indexes<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#bloom-filter-indexes\" class=\"hash-link\" aria-label=\"Direct link to Bloom Filter indexes\" title=\"Direct link to Bloom Filter indexes\">​</a></h3>\n<p>Bloom filter indexes have existed on the Hudi writers for a long time. It is one of the most performant and versatile indexes users prefer for “needle-in-a-haystack” deletes/updates or de-duplication. The index works by storing special footers in base files around min/max key ranges and a dynamic bloom filter that adapts to the file size and can automatically handle partitioning/skew on the writer's path. Hudi 1.0 introduces a newer kind of bloom filter index for Spark SQL while retaining the writer-side index as-is. The new index stores bloom filters in the Hudi metadata table and other secondary/record indexes for scalable access, even for huge tables, since the index is stored in fewer files compared to being stored alongside data files. It can be created using standard <a href=\"https://hudi.apache.org/docs/sql_ddl#create-bloom-filter-index\">SQL syntax</a>, as shown below. Subsequent queries on the indexed columns will use the bloom filters to speed up queries.</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Create a bloom filter index on the driver column of the table `hudi_table`</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">INDEX</span><span class=\"token plain\"> idx_bloom_driver </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">ON</span><span class=\"token plain\"> hudi_indexed_table </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> bloom_filters</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">driver</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\">\u000b</span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- Create a bloom filter index on the column derived from expression `lower(rider)` of the table `hudi_table`</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">INDEX</span><span class=\"token plain\"> idx_bloom_rider </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">ON</span><span class=\"token plain\"> hudi_indexed_table </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">USING</span><span class=\"token plain\"> bloom_filters</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">rider</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> OPTIONS</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\">expr</span><span class=\"token operator\">=</span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'lower'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<p>In future releases of Hudi, we aim to fully integrate the benefits of the older writer-side index into the new bloom index. Nonetheless, this demonstrates the adaptability of Hudi’s indexing system to handle different types of indexes on the table.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"partitioning-replaced-by-expression-indexes\">Partitioning replaced by Expression Indexes<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#partitioning-replaced-by-expression-indexes\" class=\"hash-link\" aria-label=\"Direct link to Partitioning replaced by Expression Indexes\" title=\"Direct link to Partitioning replaced by Expression Indexes\">​</a></h3>\n<p>An astute reader may have noticed above that the indexing is supported on a function/expression on a column. Hudi 1.0 introduces expression indexes similar to <a href=\"https://www.postgresql.org/docs/current/indexes-expressional.html\" target=\"_blank\" rel=\"noopener noreferrer\">Postgres</a> to generalize a two-decade-old relic in the data lake ecosystem - partitioning! At a high level, partitioning on the data lake divides the table into folders based on a column or a mapping function (partitioning function). When queries or operations are performed against the table, they can efficiently skip entire partitions (folders), reducing the amount of metadata and data involved. This is very effective since data lake tables span 100s of thousands of files. But, as simple as it sounds, this is one of the <a href=\"https://www.onehouse.ai/blog/knowing-your-data-partitioning-vices-on-the-data-lakehouse\" target=\"_blank\" rel=\"noopener noreferrer\">most common pitfalls</a> around performance on the data lake, where new users use it like an index by partitioning based on a high cardinality column, resulting in lots of storage partitions/tiny files and abysmal write/query performance for no good reason. Further, tying storage organization to partitioning makes it inflexible to changes.</p>\n<div style=\"text-align:center\"><img src=\"https://hudi.apache.org/assets/images/expression-index-date-partitioning.png\" alt=\"Timeline actions\"><p align=\"center\">Figure: Shows index on a date expression when a different column physically partitions data</p></div>\n<p>Hudi 1.0 treats partitions as a <a href=\"https://hudi.apache.org/docs/sql_queries#query-using-column-stats-expression-index\">coarse-grained index</a> on a column value or an expression of a column, as they should have been. To support the efficiency of skipping entire storage paths/folders, Hudi 1.0 introduces partition stats indexes that aggregate these statistics on the storage partition path level, in addition to doing so at the file level. Now, users can create different types of indexes on columns to achieve the effects of partitioning in a streamlined fashion using fewer concepts to achieve the same results. Along with support for other 1.x features, partition stats and expression indexes support will be extended to other engines like Presto, Trino, Apache Doris, and Starrocks with the 1.1 release.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"efficient-partial-updates\">Efficient Partial Updates<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#efficient-partial-updates\" class=\"hash-link\" aria-label=\"Direct link to Efficient Partial Updates\" title=\"Direct link to Efficient Partial Updates\">​</a></h3>\n<p>Managing large-scale datasets often involves making fine-grained changes to records. Hudi has long supported <a href=\"https://hudi.apache.org/docs/0.15.0/record_payload#partialupdateavropayload\">partial updates</a> to records via the record payload interface. However, this usually comes at the cost of sacrificing engine-native performance by moving away from specific objects used by engines to represent rows. As users have embraced Hudi for incremental SQL pipelines on top of dbt/Spark or Flink Dynamic Tables, there was a rise in interest in making this much more straightforward and mainstream. Hudi 1.0 introduces first-class support for <strong>partial updates</strong> at the log format level, enabling <em>MERGE INTO</em> SQL statements to modify only the changed fields of a record instead of rewriting/reprocessing the entire row.</p>\n<p>Partial updates improve query and write performance simultaneously by reducing write amplification for writes and the amount of data read by Merge-on-Read snapshot queries. It also achieves much better storage utilization due to fewer bytes stored and improved compute efficiency over existing partial update support by retaining vectorized engine-native processing. Using the 1TB Brooklyn benchmark for write performance, we observe about <strong>2.6x</strong> improvement in Merge-on-Read query performance due to an <strong>85%</strong> reduction in write amplification. For random write workloads, the gains can be much more pronounced. Below shows a second benchmark for partial updates, 1TB MOR table, 1000 partitions, 80% random updates. 3/100 columns randomly updated.</p>\n<table><thead><tr><th style=\"text-align:left\"></th><th style=\"text-align:left\">Full Record Update</th><th style=\"text-align:left\">Partial Update</th><th style=\"text-align:left\">Gains</th></tr></thead><tbody><tr><td style=\"text-align:left\"><strong>Update latency (s)</strong></td><td style=\"text-align:left\">2072</td><td style=\"text-align:left\">1429</td><td style=\"text-align:left\">1.4x</td></tr><tr><td style=\"text-align:left\"><strong>Bytes written (GB)</strong></td><td style=\"text-align:left\">891.7</td><td style=\"text-align:left\">12.7</td><td style=\"text-align:left\">70.2x</td></tr><tr><td style=\"text-align:left\"><strong>Query latency (s)</strong></td><td style=\"text-align:left\">164</td><td style=\"text-align:left\">29</td><td style=\"text-align:left\">5.7x</td></tr></tbody></table>\n<p>This also lays the foundation for managing unstructured and multimodal data inside a Hudi table and supporting <a href=\"https://github.com/apache/hudi/pull/11733\" target=\"_blank\" rel=\"noopener noreferrer\">wide tables</a> efficiently for machine learning use cases.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"merge-modes-and-custom-mergers\">Merge Modes and Custom Mergers<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#merge-modes-and-custom-mergers\" class=\"hash-link\" aria-label=\"Direct link to Merge Modes and Custom Mergers\" title=\"Direct link to Merge Modes and Custom Mergers\">​</a></h3>\n<p>One of the most unique capabilities Hudi provides is how it helps process streaming data. Specifically, Hudi has, since the very beginning, supported merging records pre-write (to reduce write amplification), during write (against an existing record in storage with the same record key) and reads (for MoR snapshot queries), using a <em>precombine</em> or <em>ordering</em> field. This helps implement <a href=\"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/\" target=\"_blank\" rel=\"noopener noreferrer\">event time processing</a> semantics, widely supported by stream processing systems, on data lakehouse storage. This helps integrate late-arriving data into Hudi tables without causing weird movement of record state back in time. For example, if an older database CDC record arrives late and gets committed as the new value, the state of the record would be incorrect even though the writes to the table themselves were serialized in some order.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"event time ordering\" src=\"https://hudi.apache.org/assets/images/event-time-ordering-merge-mode-c8164e035840388bf4290fa81ac6262a.png\" width=\"1360\" height=\"490\" class=\"img_ev3q\">\n</p><p align=\"center\">Figure: Shows EVENT_TIME_ORDERING where merging reconciles state based on the highest event_time</p><p></p>\n<p>Prior Hudi versions supported this functionality through the record payload interface with built-in support for a pre-combine field on the default payloads. Hudi 1.0 makes these two styles of processing and merging changes first class by introducing <a href=\"https://hudi.apache.org/docs/record_merger\">merge modes</a> within Hudi.</p>\n<table><thead><tr><th style=\"text-align:left\">Merge Mode</th><th style=\"text-align:left\">What does it do?</th></tr></thead><tbody><tr><td style=\"text-align:left\">COMMIT_TIME_ORDERING</td><td style=\"text-align:left\">Picks record with highest completion time/instant as final merge result  i.e., standard relational semantics or arrival time processing</td></tr><tr><td style=\"text-align:left\">EVENT_TIME_ORDERING</td><td style=\"text-align:left\">Default (for now, to ease migration).\u000bPicks record with the highest value for a user-specified ordering/precombine field as the final merge result.</td></tr><tr><td style=\"text-align:left\">CUSTOM</td><td style=\"text-align:left\">Uses a user-provided RecordMerger implementation to produce final merge result (similar to stream processing processor APIs)</td></tr></tbody></table>\n<p>Like partial update support, the new <em>RecordMerger</em> API provides a more efficient engine-native alternative to the older RecordPayload interface through native objects and vectorized processing on EVENT_TIME_ORDERING merge modes. In future versions, we intend to change the default to COMMIT_TIME_ORDERING to provide simple, out-of-the-box relational table semantics.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"non-blocking-concurrency-control-for-streaming-writes\">Non-Blocking Concurrency Control for Streaming Writes<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#non-blocking-concurrency-control-for-streaming-writes\" class=\"hash-link\" aria-label=\"Direct link to Non-Blocking Concurrency Control for Streaming Writes\" title=\"Direct link to Non-Blocking Concurrency Control for Streaming Writes\">​</a></h3>\n<p>We have expressed dissatisfaction with the optimistic concurrency control approaches employed on the data lakehouse since they appear to paint the problem with a broad brush without paying attention to the nuances of the lakehouse workloads. Specifically, contention is much more common in data lakehouses, even for Hudi, the only data lakehouse storage project capable of asynchronously compacting delta updates without failing or causing retries on the writer. Ultimately, data lakehouses are high-throughput systems, and failing concurrent writers to handle contention can waste expensive compute clusters. Streaming and high-frequency writes often require fine-grained concurrency control to prevent bottlenecks.</p>\n<p>Hudi 1.0 introduces a new <strong>non-blocking concurrency control (NBCC)</strong> designed explicitly for data lakehouse workloads, using years of experience gained supporting some of the largest data lakes on the planet in the Hudi community. NBCC enables simultaneous writing from multiple writers and compaction of the same record without blocking any involved processes. This is achieved by simply lightweight distributed locks and TrueTime semantics discussed above. (see <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md\" target=\"_blank\" rel=\"noopener noreferrer\">RFC-66</a> for more)</p>\n<div style=\"text-align:center\"><img src=\"https://hudi.apache.org/assets/images/nbcc_partial_updates.gif\" alt=\"NBCC\"><p align=\"center\">Figure: Two streaming jobs in action writing to the same records concurrently on different columns.</p></div>\n<p>NBCC operates with streaming semantics, tying together concepts from previous sections. Data necessary to compute table updates are emitted from an upstream source, and changes and partial updates can be merged in any of the merge modes above. For example, in the figure above, two independent Flink jobs enrich different table columns in parallel, a pervasive pattern seen in stream processing use cases. Check out this <a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control\" target=\"_blank\" rel=\"noopener noreferrer\">blog</a> for a full demo. We also expect to support NBCC across other compute engines in future releases.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"backwards-compatible-writing\">Backwards Compatible Writing<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#backwards-compatible-writing\" class=\"hash-link\" aria-label=\"Direct link to Backwards Compatible Writing\" title=\"Direct link to Backwards Compatible Writing\">​</a></h3>\n<p>If you are wondering: “All of this sounds cool, but how do I upgrade?” we have put a lot of thought into making that seamless. Hudi has always supported backward-compatible reads to older table versions. Table versions are stored in table properties unrelated to the software binary version. The supported way of upgrading has been to first migrate readers/query engines to new software binary versions and then upgrade the writers, which will auto-upgrade the table if there is a table version change between the old and new software binary versions. Upon community feedback, users expressed the need to be able to do upgrades on the writers without waiting on the reader side upgrades and reduce any additional coordination necessary within different teams.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Indexes\" src=\"https://hudi.apache.org/assets/images/backwards-compat-writing-6299b055646e2577964069b755ee1f3d.png\" width=\"1481\" height=\"825\" class=\"img_ev3q\">\n</p><p align=\"center\">Figure: 4-step process for painless rolling upgrades to Hudi 1.0</p><p></p>\n<p>Hudi 1.0 introduces backward-compatible writing to achieve this in 4 steps, as described above. Hudi 1.0 also automatically handles any checkpoint translation necessary as we switch to completion time-based processing semantics for incremental and CDC queries. The Hudi metadata table has to be temporarily disabled during this upgrade process but can be turned on once the upgrade is completed successfully. Please read the <a href=\"https://hudi.apache.org/releases/release-1.0.0\">release notes</a> carefully to plan your migration.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"whats-next\">What’s Next?<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#whats-next\" class=\"hash-link\" aria-label=\"Direct link to What’s Next?\" title=\"Direct link to What’s Next?\">​</a></h2>\n<p>Hudi 1.0 is a testament to the power of open-source collaboration. This release embodies the contributions of 60+ developers, maintainers, and users who have actively shaped its roadmap. We sincerely thank the Apache Hudi community for their passion, feedback, and unwavering support.</p>\n<p>The release of Hudi 1.0 is just the beginning. Our current <a href=\"https://hudi.apache.org/roadmap\">roadmap</a> includes exciting developments across the following planned releases:</p>\n<ul>\n<li><strong>1.0.1</strong>: First bug fix, patch release on top of 1.0, which hardens the functionality above and makes it easier. We intend to publish additional patch releases to aid migration to 1.0 as the bridge release for the community from 0.x.</li>\n<li><strong>1.1</strong>:  Faster writer code path rewrite, new indexes like bitmap/vector search, granular record-level change encoding, Hudi storage engine APIs, abstractions for cross-format interop.</li>\n<li><strong>1.2</strong>: Multi-table transactions, platform services for reverse streaming from Hudi etc., Multi-modal data + indexing, NBCC clustering</li>\n<li><strong>2.0</strong>: Server components for DLMS, caching and metaserver functionality.</li>\n</ul>\n<p>Hudi releases are drafted collaboratively by the community. If you don’t see something you like here, please help shape the roadmap together.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"get-started-with-apache-hudi-10\">Get Started with Apache Hudi 1.0<a href=\"https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0#get-started-with-apache-hudi-10\" class=\"hash-link\" aria-label=\"Direct link to Get Started with Apache Hudi 1.0\" title=\"Direct link to Get Started with Apache Hudi 1.0\">​</a></h2>\n<p>Are you ready to experience the future of data lakehouses? Here’s how you can dive into Hudi 1.0:</p>\n<ul>\n<li>Documentation: Explore Hudi’s <a href=\"https://hudi.apache.org/docs/overview\">Documentation</a> and learn the <a href=\"https://hudi.apache.org/docs/hudi_stack\">concepts</a>.</li>\n<li>Quickstart Guide: Follow the <a href=\"https://hudi.apache.org/docs/quick-start-guide\">Quickstart Guide</a> to set up your first Hudi project.</li>\n<li>Upgrading from a previous version?  Follow the <a href=\"https://hudi.apache.org/releases/release-1.0.0#migration-guide\">migration guide</a> and contact the Hudi OSS community for help.</li>\n<li>Join the Community: Participate in discussions on the <a href=\"https://hudi.apache.org/community/get-involved/\" target=\"_blank\" rel=\"noopener noreferrer\">Hudi Mailing List</a>, <a href=\"https://join.slack.com/t/apache-hudi/shared_invite/zt-2ggm1fub8-_yt4Reu9djwqqVRFC7X49g\" target=\"_blank\" rel=\"noopener noreferrer\">Slack</a> and <a href=\"https://github.com/apache/hudi/issues\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>.</li>\n<li>Follow us on social media: <a href=\"https://www.linkedin.com/company/apache-hudi/?viewAsMember=true\" target=\"_blank\" rel=\"noopener noreferrer\">Linkedin</a>, <a href=\"https://twitter.com/ApacheHudi\" target=\"_blank\" rel=\"noopener noreferrer\">X/Twitter</a>.</li>\n</ul>\n<p>We can’t wait to see what you build with Apache Hudi 1.0. Let’s work together to shape the future of data lakehouses!</p>\n<p>Crafted with passion for the Apache Hudi community.</p>",
            "url": "https://hudi.apache.org/blog/2024/12/16/announcing-hudi-1-0-0",
            "title": "Announcing Apache Hudi 1.0 and the Next Generation of Data Lakehouses",
            "summary": "Overview",
            "date_modified": "2024-12-16T00:00:00.000Z",
            "author": {
                "name": "Vinoth Chandar"
            },
            "tags": [
                "timeline",
                "design",
                "release",
                "streaming ingestion",
                "multi-writer",
                "concurrency-control",
                "blog"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"introduction\">Introduction<a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#introduction\" class=\"hash-link\" aria-label=\"Direct link to Introduction\" title=\"Direct link to Introduction\">​</a></h2>\n<p>In streaming ingestion scenarios, there are plenty of use cases that require concurrent ingestion from multiple streaming sources.\nThe user can union all the upstream source inputs into one downstream table to collect the records for unified access across federated queries.\nAnother very common scenario is multiple stream sources joined together to supplement dimensions of the records to build a wide-dimension table where each source\nstream is taking records with partial table schema fields. Common and strong demand for multi-stream concurrent ingestion has always been there.\nThe Hudi community has collected so many feedbacks from users ever since the day Hudi supported streaming ingestion and processing.</p>\n<p>Starting from <a href=\"https://hudi.apache.org/releases/release-1.0.0\" target=\"_blank\" rel=\"noopener noreferrer\">Hudi 1.0.0</a>, we are thrilled to announce a new general-purpose\nconcurrency model for Apache Hudi - the Non-blocking Concurrency Control (NBCC)- aimed at the stream processing or high-contention/frequent writing scenarios.\nIn contrast to <a href=\"https://hudi.apache.org/blog/2021/12/16/lakehouse-concurrency-control-are-we-too-optimistic/\">Optimistic Concurrency Control</a>, where writers abort the transaction\nif there is a hint of contention, this innovation allows multiple streaming writes to the same Hudi table without any overhead of conflict resolution, while\nkeeping the semantics of <a href=\"https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/\" target=\"_blank\" rel=\"noopener noreferrer\">event-time ordering</a> found in streaming systems, along with\nasynchronous table service such as compaction, archiving and cleaning.</p>\n<p>NBCC works seamlessly without any new infrastructure or operational overhead. In the subsequent sections of this blog, we will give a brief introduction to Hudi's internals\nabout the data file layout and TrueTime semantics for time generation, a pre-requisite for discussing NBCC. Following that, we will delve into the design and workflows of NBCC,\nand then a simple SQL demo to show the NBCC related config options. The blog will conclude with insights into future work for NBCC.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"older-design\">Older Design<a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#older-design\" class=\"hash-link\" aria-label=\"Direct link to Older Design\" title=\"Direct link to Older Design\">​</a></h2>\n<p>It's important to understand the Hudi <a href=\"https://hudi.apache.org/docs/next/storage_layouts\">storage layout</a> and it evolves/manages data versions. In older release before 1.0.0,\nHudi organizes the data files with units as <code>FileGroup</code>. Each file group contains multiple <code>FileSlice</code>s. Every compaction on this file group generates a new file slice.\nEach file slice may comprise an optional base file(columnar file format like Apache Parquet or ORC) and multiple log files(row file format in Apache Avro or Parquet).</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/legacy_file_layout.png\" alt=\"Legacy file layout\" width=\"800\" align=\"middle\">\n<p>The timestamp in the base file name is the instant time of the compaction that writes it, it is also called as \"requested instant time\" in Hudi's notion.\nThe timestamp in the log file name is the same timestamp as the current file slice base instant time. Data files with the same instant time belong to one file slice.\nIn effect, a file group represented a linear ordered sequence of base files (checkpoints) followed by logs files (deltas), followed by base files (checkpoints).</p>\n<p>The instant time naming convention in log files becomes a hash limitation in concurrency mode. Each log file contains incremental changes from\nmultiple commits. Each writer needs to query the file layout to get the base instant time and figure out the full file name before flushing the records.\nA more severe problem is the base instant time can be variable with the async compaction pushing forward. In order to make the base instant time deterministic for the log writers, Hudi\nforces the schedule sequence between a write commit and compaction scheduling: a compaction can be scheduled only if there is no ongoing ingestion into the Hudi table. Without this, a log file\ncan be written with a wrong base instant time which could introduce data loss. This means a compaction scheduling could block all the writers in concurrency mode.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"nbcc-design\">NBCC Design<a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#nbcc-design\" class=\"hash-link\" aria-label=\"Direct link to NBCC Design\" title=\"Direct link to NBCC Design\">​</a></h2>\n<p>In order to resolve these pains, since 1.0.0, Hudi introduces a new storage layout based on both requested and completion times of actions, viewing them as an interval.\nEach commit in 1.x Hudi has two <a href=\"https://hudi.apache.org/docs/next/timeline\">important notions of time</a>: instant time(or requested time) and completion time.\nAll the generated timestamp are globally monotonically increasing. Instead of putting the base instant time in the log file name, Hudi now just uses the requested instant time\nof the write. During file slicing, Hudi queries the completion time for each log file with the instant time, and we have a new rule for file slicing:</p>\n<p><em>A log file belongs to the file slice with the maximum base requested time smaller than(or equals with) it's completion time.</em>[^1]</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/new_file_layout.png\" alt=\"New file layout\" width=\"800\" align=\"middle\">\n<p>With the flexibility of the new file layout, the overhead of querying base instant time is eliminated for log writers and a compaction can be scheduled anywhere with any instant time.\nSee <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md\" target=\"_blank\" rel=\"noopener noreferrer\">RFC-66</a> for more.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"true-time-api\">True Time API<a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#true-time-api\" class=\"hash-link\" aria-label=\"Direct link to True Time API\" title=\"Direct link to True Time API\">​</a></h3>\n<p>In order to ensure the monotonicity of timestamp generation, Hudi introduces the \"<a href=\"https://hudi.apache.org/docs/next/timeline#timeline-components\">TrueTime API</a>\" since 1.x release.\nBasically there are two ways to make the time generation monotonically increasing, inline with TrueTime semantics:</p>\n<ul>\n<li>A global lock to guard the time generation with mutex, along with a wait for an estimated max allowed clock skew on distributed hosts;</li>\n<li>Globally synchronized time generation service, e.g. Google Spanner Time Service, the service itself can ensure the monotonicity.</li>\n</ul>\n<p>Hudi now implements the \"TrueTime\" semantics with the first solution, a configurable max waiting time is supported.</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"lsm-timeline\">LSM timeline<a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#lsm-timeline\" class=\"hash-link\" aria-label=\"Direct link to LSM timeline\" title=\"Direct link to LSM timeline\">​</a></h3>\n<p>The new file layout requires efficient queries from instant time to get the completion time. Hudi re-implements the archived timeline since 1.x, the\nnew archived timeline data files are organized as <a href=\"https://hudi.apache.org/docs/next/timeline#lsm-timeline-history\">an LSM tree</a> to support fast time range filtering queries with instant time data-skipping on it.</p>\n<img src=\"https://hudi.apache.org/assets/images/blog/non-blocking-concurrency-control/lsm_archive_timeline.png\" alt=\"LSM archive timeline\" align=\"middle\">\n<p>With the powerful new file layout, it is quite straight-forward to implement non-blocking concurrency control. The function is implemented with the simple bucket index on MOR table for Flink.\nThe bucket index ensures fixed record key to file group mappings for multiple workloads. The log writer writes the records into avro logs and the compaction table service would take care of\nthe conflict resolution. Because each log file name contains the instant time and each record contains the event time ordering field, Hudi reader can merge the records either\nwith natural order(processing time sequence) or event time order.</p>\n<p>The concurrency mode should be configured as <code>NON_BLOCKING_CONCURRENCY_CONTROL</code>, you can enable the table services on one job and disable it for the others.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"flink-sql-demo\">Flink SQL demo<a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#flink-sql-demo\" class=\"hash-link\" aria-label=\"Direct link to Flink SQL demo\" title=\"Direct link to Flink SQL demo\">​</a></h2>\n<p>Here is a demo to show 2 pipelines that ingest into the same downstream table, the two sink table views share the same table path.</p>\n<div class=\"language-sql codeBlockContainer_Ckt0 theme-code-block\" style=\"--prism-color:#F8F8F2;--prism-background-color:#282A36\"><div class=\"codeBlockContent_biex\"><pre tabindex=\"0\" class=\"prism-code language-sql codeBlock_bY9V thin-scrollbar\" style=\"color:#F8F8F2;background-color:#282A36\"><code class=\"codeBlockLines_e6Vv\"><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- NB-CC demo</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- The source table</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">CREATE</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">TABLE</span><span class=\"token plain\"> sourceT </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  uuid </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  name </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  age </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">int</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">timestamp</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token identifier punctuation\" style=\"color:rgb(248, 248, 242)\">`</span><span class=\"token identifier\">partition</span><span class=\"token identifier punctuation\" style=\"color:rgb(248, 248, 242)\">`</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">as</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'par1'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">WITH</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'connector'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'datagen'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'rows-per-second'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'200'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- table view for writer1</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">create</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">table</span><span class=\"token plain\"> t1</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  uuid </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  name </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  age </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">int</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">timestamp</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token identifier punctuation\" style=\"color:rgb(248, 248, 242)\">`</span><span class=\"token identifier\">partition</span><span class=\"token identifier punctuation\" style=\"color:rgb(248, 248, 242)\">`</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">with</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'connector'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hudi'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'path'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'/Users/chenyuzhao/workspace/hudi-demo/t1'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'table.type'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'MERGE_ON_READ'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'index.type'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'BUCKET'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.write.concurrency.mode'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'NON_BLOCKING_CONCURRENCY_CONTROL'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'write.tasks'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">insert</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">into</span><span class=\"token plain\"> t1</span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">/*+options('metadata.enabled'='true')*/</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">select</span><span class=\"token plain\"> </span><span class=\"token operator\">*</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">from</span><span class=\"token plain\"> sourceT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- table view for writer2</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- compaction and cleaning are disabled because writer1 has taken care of it.</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">create</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">table</span><span class=\"token plain\"> t1_2</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  uuid </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  name </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  age </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">int</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  ts </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">timestamp</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token identifier punctuation\" style=\"color:rgb(248, 248, 242)\">`</span><span class=\"token identifier\">partition</span><span class=\"token identifier punctuation\" style=\"color:rgb(248, 248, 242)\">`</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">varchar</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">with</span><span class=\"token plain\"> </span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">(</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'connector'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hudi'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'path'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'/Users/chenyuzhao/workspace/hudi-demo/t1'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'table.type'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'MERGE_ON_READ'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'index.type'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'BUCKET'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'hoodie.write.concurrency.mode'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'NON_BLOCKING_CONCURRENCY_CONTROL'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'write.tasks'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'2'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'compaction.schedule.enabled'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'false'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'compaction.async.enabled'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'false'</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">,</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\">  </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'clean.async.enabled'</span><span class=\"token plain\"> </span><span class=\"token operator\">=</span><span class=\"token plain\"> </span><span class=\"token string\" style=\"color:rgb(255, 121, 198)\">'false'</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">)</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\" style=\"display:inline-block\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token comment\" style=\"color:rgb(98, 114, 164)\">-- executes the ingestion workloads</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">insert</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">into</span><span class=\"token plain\"> t1 </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">select</span><span class=\"token plain\"> </span><span class=\"token operator\">*</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">from</span><span class=\"token plain\"> sourceT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><span class=\"token plain\"></span><br></span><span class=\"token-line\" style=\"color:#F8F8F2\"><span class=\"token plain\"></span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">insert</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">into</span><span class=\"token plain\"> t1_2 </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">select</span><span class=\"token plain\"> </span><span class=\"token operator\">*</span><span class=\"token plain\"> </span><span class=\"token keyword\" style=\"color:rgb(189, 147, 249);font-style:italic\">from</span><span class=\"token plain\"> sourceT</span><span class=\"token punctuation\" style=\"color:rgb(248, 248, 242)\">;</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button type=\"button\" aria-label=\"Copy code to clipboard\" title=\"Copy\" class=\"clean-btn\"><span class=\"copyButtonIcons_eSgA\" aria-hidden=\"true\"><svg viewBox=\"0 0 24 24\" class=\"copyButtonIcon_y97N\"><path fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"></path></svg><svg viewBox=\"0 0 24 24\" class=\"copyButtonSuccessIcon_LjdS\"><path fill=\"currentColor\" d=\"M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z\"></path></svg></span></button></div></div></div>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"future-roadmap\">Future Roadmap<a href=\"https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control#future-roadmap\" class=\"hash-link\" aria-label=\"Direct link to Future Roadmap\" title=\"Direct link to Future Roadmap\">​</a></h2>\n<p>While non-blocking concurrency control is a very powerful feature for streaming users, it is a general solution for multiple writer conflict resolution,\nhere are some plans that improve the Hudi core features:</p>\n<ul>\n<li>NBCC support for metadata table</li>\n<li>NBCC for clustering</li>\n<li>NBCC for other index type</li>\n</ul>\n<hr>\n<p>[^1] <a href=\"https://github.com/apache/hudi/blob/master/rfc/rfc-66/rfc-66.md\" target=\"_blank\" rel=\"noopener noreferrer\">RFC-66</a> well-explained the completion time based file slicing with a pseudocode.</p>",
            "url": "https://hudi.apache.org/blog/2024/12/06/non-blocking-concurrency-control",
            "title": "Introducing Hudi's Non-blocking Concurrency Control for streaming, high-frequency writes",
            "summary": "Introduction",
            "date_modified": "2024-12-06T00:00:00.000Z",
            "author": {
                "name": "Danny Chan"
            },
            "tags": [
                "design",
                "streaming ingestion",
                "multi-writer",
                "concurrency-control",
                "blog"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling",
            "content_html": "<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"introduction\">Introduction<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#introduction\" class=\"hash-link\" aria-label=\"Direct link to Introduction\" title=\"Direct link to Introduction\">​</a></h2>\n<p>In today’s data-driven world, managing large volumes of data efficiently is crucial. One of the standout features of Apache Hudi is its ability to handle small files during data writes, which significantly optimizes both performance and cost. In this post, we’ll explore how Hudi’s auto file sizing, powered by a unique bin packing algorithm, can transform your data processing workflows.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"understanding-small-file-challenges\">Understanding Small File Challenges<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#understanding-small-file-challenges\" class=\"hash-link\" aria-label=\"Direct link to Understanding Small File Challenges\" title=\"Direct link to Understanding Small File Challenges\">​</a></h2>\n<p>In big data environments, small files can pose a major challenge. Some major use-cases which can create lot of small files -</p>\n<ul>\n<li><strong>Streaming Workloads</strong> :\nWhen data is ingested in micro-batches, as is common in streaming workloads, the resulting files tend to be small. This can lead to a significant number of small files, especially for high-throughput streaming applications.</li>\n<li><strong>High-Cardinality Partitioning</strong> :\nExcessive partitioning, particularly on columns with high cardinality, can create a large number of small files. This can be especially problematic when dealing with large datasets and complex data schemas.</li>\n</ul>\n<p>These small files can lead to several inefficiencies that can include increased metadata overhead, degraded read performance, and higher storage costs, particularly when using cloud storage solutions like Amazon S3.</p>\n<ul>\n<li><strong>Increased Metadata Overhead</strong> :\nMetadata is data about data, including information such as file names, sizes, creation dates, and other attributes that help systems manage and locate files. Each file, no matter how small, requires metadata to be tracked and managed. In environments where numerous small files are created, the amount of metadata generated can skyrocket. For instance, if a dataset consists of thousands of tiny files, the system must maintain metadata for each of these files. This can overwhelm metadata management systems, leading to longer lookup times and increased latency when accessing files.</li>\n<li><strong>Degraded Read Performance</strong> :\nReading data from storage typically involves input/output (I/O) operations, which can be costly in terms of time and resources. When files are small, the number of I/O operations increases, as each small file needs to be accessed individually. This scenario can create bottlenecks, particularly in analytical workloads where speed is critical. Querying a large number of small files may result in significant delays, as the system spends more time opening and reading each file than processing the data itself.</li>\n<li><strong>Higher Cloud Costs</strong> :\nMany cloud storage solutions, like Amazon S3, charge based on the total amount of data stored as well as the number of requests made. With numerous small files, not only does the total storage requirement increase, but the number of requests to access these files also grows. Each small file incurs additional costs due to the overhead associated with managing and accessing them. This can add up quickly, leading to unexpectedly high storage bills.</li>\n<li><strong>High Query Load</strong> :\nMultiple teams are querying these tables for various dashboards, ad-hoc analyses, and machine learning tasks. This leads to a high number of concurrent queries, including Spark jobs, which can significantly impact performance. All those queries/jobs will take a hit on both performance and cost.</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"impact-of-small-file\">Impact of Small File<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#impact-of-small-file\" class=\"hash-link\" aria-label=\"Direct link to Impact of Small File\" title=\"Direct link to Impact of Small File\">​</a></h3>\n<p>To demonstrate the impact of small files, we conducted a benchmarking using AWS EMR.\nDataset Used - TPC-DS 1 TB dataset ( <a href=\"https://www.tpc.org/tpcds/\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.tpc.org/tpcds/</a> )\nCluster Configurations - 10 nodes (m5.4xlarge)\nSpark Configurations - Executors: 10 (16 cores 32 GB memory)\nDataset Generation - We generated two types of datasets in parquet format</p>\n<ul>\n<li>Optimized File Sizes which had ~100 MB sized files</li>\n<li>Small File Sizes which had ~5-10 MB sized files\nExecution and Results</li>\n<li>We executed 3 rounds of 99 standard TPC-DS queries on both datasets and measured the time taken by the queries.</li>\n<li>The results indicated that queries executed on small files were, on average, 30% slower compared to those executed on optimized file sizes.</li>\n</ul>\n<p>The following chart illustrates the average runtimes for the 99 queries across each round.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"Impact of Small Files\" src=\"https://hudi.apache.org/assets/images/2024-11-19-automated-small-file-handling-benchmarks-5340e7e5e0e586c3803f6e06796b5daf.png\" width=\"3188\" height=\"1844\" class=\"img_ev3q\"></p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-table-formats-solve-this-problem\">How table formats solve this problem<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#how-table-formats-solve-this-problem\" class=\"hash-link\" aria-label=\"Direct link to How table formats solve this problem\" title=\"Direct link to How table formats solve this problem\">​</a></h2>\n<p>When it comes to managing small files in table formats, there are two primary strategies:</p>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"ingesting-data-as-is-and-optimizing-post-ingestion-\"><strong>Ingesting Data As-Is and Optimizing Post-Ingestion</strong> :<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#ingesting-data-as-is-and-optimizing-post-ingestion-\" class=\"hash-link\" aria-label=\"Direct link to ingesting-data-as-is-and-optimizing-post-ingestion-\" title=\"Direct link to ingesting-data-as-is-and-optimizing-post-ingestion-\">​</a></h3>\n<p>In this approach, data, including small files, is initially ingested without immediate processing. After ingestion, various technologies provide functionalities to merge these small files into larger, more efficient partitions:</p>\n<ul>\n<li>Hudi uses clustering to manage small files.</li>\n<li>Delta Lake utilizes the OPTIMIZE command.</li>\n<li>Iceberg offers the rewrite_data_files function.</li>\n</ul>\n<h4 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"pros\">Pros:<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#pros\" class=\"hash-link\" aria-label=\"Direct link to Pros:\" title=\"Direct link to Pros:\">​</a></h4>\n<ul>\n<li>Writing small files directly accelerates the ingestion process, enabling quick data availability—especially beneficial for real-time or near-real-time applications.</li>\n<li>The initial write phase involves less data manipulation, as small files are simply appended. This streamlines workflows and eases the management of incoming data streams.</li>\n</ul>\n<h4 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"cons\">Cons:<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#cons\" class=\"hash-link\" aria-label=\"Direct link to Cons:\" title=\"Direct link to Cons:\">​</a></h4>\n<ul>\n<li>Until clustering or optimization is performed, small files may be exposed to readers, which can significantly slow down queries and potentially violate read SLAs.</li>\n<li>Just like with read performance, exposing small files to readers can lead to a high number of cloud storage API calls, which can increase cloud costs significantly.</li>\n<li>Managing table service jobs can become cumbersome. These jobs often can't run in parallel with ingestion tasks, leading to potential delays and resource contention.</li>\n</ul>\n<h3 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"managing-small-files-during-ingestion-only-\"><strong>Managing Small Files During Ingestion Only</strong> :<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#managing-small-files-during-ingestion-only-\" class=\"hash-link\" aria-label=\"Direct link to managing-small-files-during-ingestion-only-\" title=\"Direct link to managing-small-files-during-ingestion-only-\">​</a></h3>\n<p>Hudi offers a unique functionality that can handle small files during the ingestion only, ensuring that only larger files are stored in the table. This not only optimizes read performance but also significantly reduces storage costs.\nBy eliminating small files from the lake, Hudi addresses key challenges associated with data management, providing a streamlined solution that enhances both performance and cost efficiency.</p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"how-hudi-helps-in-small-file-handling-during-ingestion\">How Hudi helps in small file handling during ingestion<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#how-hudi-helps-in-small-file-handling-during-ingestion\" class=\"hash-link\" aria-label=\"Direct link to How Hudi helps in small file handling during ingestion\" title=\"Direct link to How Hudi helps in small file handling during ingestion\">​</a></h2>\n<p>Hudi automatically manages file sizing during insert and upsert operations. It employs a bin packing algorithm to handle small files effectively. A bin packing algorithm is a technique used to optimize file storage by grouping files of varying sizes into fixed-size containers, often referred to as \"bins.\" This strategy aims to minimize the number of bins required to store all files efficiently. When writing data, Hudi identifies file groups of small files and merges new data into the same  group, resulting in optimized file sizes.</p>\n<p>The diagram above illustrates how Hudi employs a bin packing algorithm to manage small files while using default parameters: a small file limit of 100 MB and a maximum file size of 120 MB.</p>\n<p><img decoding=\"async\" loading=\"lazy\" alt=\"  \" src=\"https://hudi.apache.org/assets/images/2024-11-19-automated-small-file-handling-process-676b9be484af36088162dfaf6a219a1f.png\" width=\"1350\" height=\"632\" class=\"img_ev3q\"></p>\n<p>Initially, the table contains the following files: F1 (110 MB), F2 (60 MB), F3 (20 MB), and F4 (20 MB).\nAfter processing a batch-1 of 150 MB, F2, F3, and F4 will all be classified as small files since they each fall below the 100 MB threshold. The first 60 MB will be allocated to F2, increasing its size to 120 MB. The remaining 90 MB will be assigned to F3, bringing its total to 110 MB.\nAfter processing batch-2 of 150 MB, only F4 will be classified as a small file. F3, now at 110 MB, will not be considered a small file since it exceeds the 100 MB limit. Therefore, an additional 100 MB will be allocated to F4, increasing its size to 120 MB, while the remaining 50 MB will create a new file of 50 MB.\nWe can refer this blog for in-depth details of the functionality  - <a href=\"https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/\" target=\"_blank\" rel=\"noopener noreferrer\">https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/</a></p>\n<p>We use following configs to configure this -</p>\n<ul>\n<li>\n<p><strong>Hoodie.parquet.max.file.size (Default 128 MB)</strong>\nThis setting specifies the target size, in bytes, for Parquet files generated during Hudi write phases. The writer will attempt to create files that approach this target size. For example, if an existing file is 80 MB, the writer will allocate only 40 MB to that particular file group.</p>\n</li>\n<li>\n<p><strong>Hoodie.parquet.small.file.limit (Default 100 MB)</strong>\nThis setting defines the maximum file size for a data file to be classified as a small file. Files below this threshold are considered small files, prompting the system to allocate additional records to their respective file groups in subsequent write phases.</p>\n</li>\n<li>\n<p><strong>hoodie.copyonwrite.record.size.estimate (Default 1024)</strong>\nThis setting represents the estimated average size of a record. If not explicitly specified, Hudi will dynamically compute this estimate based on commit metadata. Accurate record size estimation is essential for determining insert parallelism and efficiently bin-packing inserts into smaller files.</p>\n</li>\n<li>\n<p><strong>hoodie.copyonwrite.insert.split.size (Default 500000)</strong>\nThis setting determines the number of records inserted into each partition or bucket during a write operation. The default value is based on the assumption of 100MB files with at least 1KB records, resulting in approximately 100,000 records per file. To accommodate potential variations, we overprovision to 500,000 records. As long as auto-tuning of splits is turned on, this only affects the first write, where there is no history to learn record sizes from.</p>\n</li>\n<li>\n<p><strong>hoodie.merge.small.file.group.candidates.limit (Default1)</strong>\nThis setting specifies the maximum number of file groups whose base files meet the small-file limit that can be considered for appending records during an upsert operation. This parameter is applicable only to Merge-On-Read (MOR) tables.</p>\n</li>\n</ul>\n<p>We can refer this blog to understand internal functionality how it works -\n<a href=\"https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write\" target=\"_blank\" rel=\"noopener noreferrer\">https://hudi.apache.org/blog/2021/03/01/hudi-file-sizing/#during-write-vs-after-write</a></p>\n<h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"conclusion\">Conclusion<a href=\"https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling#conclusion\" class=\"hash-link\" aria-label=\"Direct link to Conclusion\" title=\"Direct link to Conclusion\">​</a></h2>\n<p>Hudi's innovative approach to managing small files during ingestion positions it as a compelling choice in the lakehouse landscape. By automatically merging small files at the time of ingestion, it optimizes storage costs and enhances read performance, and alleviates users from the operational burden of maintaining their tables in an optimized state.</p>\n<p>Unleash the power of Apache Hudi for your big data challenges! Head over to <a href=\"https://hudi.apache.org/\" target=\"_blank\" rel=\"noopener noreferrer\">https://hudi.apache.org/</a> and dive into the quickstarts to get started. Want to learn more? Join our vibrant Hudi community! Attend the monthly Community Call or hop into the Apache Hudi Slack to ask questions and gain deeper insights.</p>",
            "url": "https://hudi.apache.org/blog/2024/11/19/automated-small-file-handling",
            "title": "Hudi’s Automatic File Sizing Delivers Unmatched Performance",
            "summary": "Introduction",
            "date_modified": "2024-11-19T00:00:00.000Z",
            "author": {
                "name": "Aditya Goenka"
            },
            "tags": [
                "Data Lake",
                "Apache Hudi"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/11/12/record-level-indexing-in-apache-hudi",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://medium.com/@prasadpal107/record-level-indexing-in-apache-hudi-0615804608ec\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/11/12/record-level-indexing-in-apache-hudi",
            "title": "Record Level Indexing in Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-11-12T00:00:00.000Z",
            "author": {
                "name": "Bibhu Pala"
            },
            "tags": [
                "blog",
                "apache hudi",
                "record index",
                "record level index",
                "medium"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/11/12/storing-200-billion-entities-notions",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://blog.bytebytego.com/p/storing-200-billion-entities-notions\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/11/12/storing-200-billion-entities-notions",
            "title": "Storing 200 Billion Entities: Notion’s Data Lake Project",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-11-12T00:00:00.000Z",
            "author": {
                "name": "ByteByteGo"
            },
            "tags": [
                "blog",
                "apache hudi",
                "use-case",
                "bytebytego"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://opstree.com/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi-choosing-the-right-storage-strategy/\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/11/12/understanding-cow-and-mor-in-apache-hudi",
            "title": "Understanding COW and MOR in Apache Hudi: Choosing the Right Storage Strategy",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-11-12T00:00:00.000Z",
            "author": {
                "name": "Deepak Nishad"
            },
            "tags": [
                "blog",
                "apache hudi",
                "cow",
                "mor",
                "opstree"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/27/I-spent-5-hours-exploring-the-story-behind-Apache-Hudi",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://blog.det.life/i-spent-5-hours-exploring-the-story-behind-apache-hudi-dacad829394d\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/27/I-spent-5-hours-exploring-the-story-behind-Apache-Hudi",
            "title": "I spent 5 hours exploring the story behind Apache Hudi.",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-27T00:00:00.000Z",
            "author": {
                "name": "Vu Trinh"
            },
            "tags": [
                "blog",
                "apache hudi",
                "beginner",
                "det"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/26/moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://www.linkedin.com/pulse/moving-large-tables-from-snowflake-s3-using-copy-command-soumil-shah-csdse/?trackingId=8qFtCUc3R7CAo%2BP883rgUA%3D%3D\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/26/moving-large-tables-from-snowflake-to-s3-using-the-copy-into-command-and-hudi",
            "title": "Moving Large Tables from Snowflake to S3 Using the COPY INTO Command and Hudi Bootstrapping to Build Data Lakes | Hands-On Labs",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-26T00:00:00.000Z",
            "author": {
                "name": "Soumil Shah"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "aws s3",
                "bootstrap",
                "linkedin"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/23/Using-Apache-Hudi-with-Apache-Flink",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/tutorial-hudi-for-flink.html\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/23/Using-Apache-Hudi-with-Apache-Flink",
            "title": "Using Apache Hudi with Apache Flink",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-23T00:00:00.000Z",
            "author": {
                "name": "amazon"
            },
            "tags": [
                "blog",
                "apache hudi",
                "apache flink",
                "beginner",
                "aws",
                "amazon"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/23/mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://medium.com/itversity/understanding-open-table-formats-a-comprehensive-guide-ba6f072167fb\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/23/mastering-open-table-formats-a-guide-to-apache-iceberg-hudi-and-delta-lake",
            "title": "Mastering Open Table Formats: A Guide to Apache Iceberg, Hudi, and Delta Lake",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-23T00:00:00.000Z",
            "author": {
                "name": "Naresh Dulam"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "Apache Iceberg",
                "Delta Lake",
                "comparison",
                "medium"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/22/exploring-time-travel-queries-in-apache-hudi",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://opstree.com/blog/2024/10/22/time-travel-queries-in-apache-hudi/\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/22/exploring-time-travel-queries-in-apache-hudi",
            "title": "Exploring Time Travel Queries in Apache Hudi",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-22T00:00:00.000Z",
            "author": {
                "name": "Ramneek Kaur"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "time travel query",
                "opstree"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/14/streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://www.antstack.com/blog/Streaming-DynamoDB-Data-into-a-Hudi-Table/\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/14/streaming-dynamodb-data-into-a-hudi-table-aws-glue-in-action",
            "title": "Streaming DynamoDB Data into a Hudi Table: AWS Glue in Action",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-14T00:00:00.000Z",
            "author": {
                "name": "Rahul Kumar"
            },
            "tags": [
                "how-to",
                "Apache Hudi",
                "amazon s3",
                "aws glue",
                "amazon kinesis",
                "amazon dynamodb",
                "antstack"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/07/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://blog.forefathers.io/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures-52eec62b29e8\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/07/iceberg-vs-delta-lake-vs-hudi-a-comparative-look-at-lakehouse-architectures",
            "title": "Iceberg vs. Delta Lake vs. Hudi: A Comparative Look at Lakehouse Architectures",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-07T00:00:00.000Z",
            "author": {
                "name": "Abdelkbir Armel"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "Apache Iceberg",
                "Delta Lake",
                "comparison",
                "forefathers"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/07/mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://www.linkedin.com/pulse/mastering-slowly-changing-dimensions-apache-hudi-spark-sameer-shaik-7zkjf/?trackingId=1qCeO8FIRJy32LcpHIvy3Q%3D%3D\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/07/mastering-slowly-changing-dimensions-with-apache-hudi-and-spark-sql",
            "title": "Mastering Slowly Changing Dimensions with Apache Hudi & Spark SQL",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-07T00:00:00.000Z",
            "author": {
                "name": "Sameer Shaik"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "scd1",
                "scd2",
                "scd3",
                "spark-sql",
                "linkedin"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/10/02/apache-hudi-spark-and-minio-hands-on-lab-in-docker",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://blog.devgenius.io/apache-hudi-spark-and-minio-hands-on-lab-in-docker-f1daa099ccd0\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/10/02/apache-hudi-spark-and-minio-hands-on-lab-in-docker",
            "title": "Apache Hudi, Spark and Minio: Hands-on Lab in Docker",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-10-02T00:00:00.000Z",
            "author": {
                "name": "Sanjeet Shukla"
            },
            "tags": [
                "how-to",
                "Apache Hudi",
                "Apache Spark",
                "Minio",
                "docker",
                "devgenius"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/09/30/change-query-support-in-apache-hudi-0-15",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://jack-vanlightly.com/analyses/2024/9/27/change-query-support-in-apache-hudi\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/09/30/change-query-support-in-apache-hudi-0-15",
            "title": "Change query support in Apache Hudi (0.15)",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-09-30T00:00:00.000Z",
            "author": {
                "name": "Jack Vanlightly"
            },
            "tags": [
                "blog",
                "apache hudi",
                "CDC",
                "Change Data Capture",
                "jack-vanlightly"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/09/24/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://lakefs.io/blog/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/09/24/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared",
            "title": "Hudi, Iceberg and Delta Lake: Data Lake Table Formats Compared",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-09-24T00:00:00.000Z",
            "author": {
                "name": "Oz Katz"
            },
            "tags": [
                "blog",
                "apache hudi",
                "apache iceberg",
                "delta lake",
                "comparison",
                "lakefs"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/09/22/hands-on-with-apache-hudi-and-spark",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://blog.devgenius.io/hands-on-with-apache-hudi-ce45869b5eff\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/09/22/hands-on-with-apache-hudi-and-spark",
            "title": "Hands-on with Apache Hudi and Spark",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-09-22T00:00:00.000Z",
            "author": {
                "name": "Sanjeet Shukla"
            },
            "tags": [
                "blog",
                "Apache Hudi",
                "Apache Spark",
                "devgenius"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/09/17/how-apache-hudi-transformed-yuno-s-data-lake",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://www.y.uno/post/how-apache-hudi-transformed-yunos-data-lake\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/09/17/how-apache-hudi-transformed-yuno-s-data-lake",
            "title": "How Apache Hudi transformed Yuno’s data lake",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-09-17T00:00:00.000Z",
            "author": {
                "name": "Nahuel Leandro Mazzitelli"
            },
            "tags": [
                "blog",
                "apache hudi",
                "cow",
                "mor",
                "record index",
                "record level index",
                "clustering",
                "cleaning",
                "bloom index",
                "fiel sizing",
                "y.uno"
            ]
        },
        {
            "id": "https://hudi.apache.org/blog/2024/09/14/Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond",
            "content_html": "<span>Redirecting... please wait!! <!-- -->or click <a href=\"https://vutr.substack.com/p/ubers-big-data-revolution-from-mysql\">here</a></span>",
            "url": "https://hudi.apache.org/blog/2024/09/14/Ubers-Big-Data-Revolution-From-MySQL-to-Hadoop-and-Beyond",
            "title": "Uber’s Big Data Revolution: From MySQL to Hadoop and Beyond",
            "summary": "Redirecting... please wait!!",
            "date_modified": "2024-09-14T00:00:00.000Z",
            "author": {
                "name": "Vu Trinh"
            },
            "tags": [
                "blog",
                "apache hudi",
                "use-case",
                "substack"
            ]
        }
    ]
}